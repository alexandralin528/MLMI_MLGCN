{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement MLGCN on ODIR dataset\n",
    "\n",
    "* **Link to the paper:** https://arxiv.org/pdf/1912.11757.pdf \n",
    "\n",
    "Start with graph construction from the given patients' data. To have the similar input to the load function we build for BlogCatalog dataset, we will create two csv files, `edges.csv` and `group-edges.csv`.\n",
    "\n",
    "The patients are conected if they have age difference less than 5-year-old, this information is than recorded in `edges.csv`, the first column is the patient node and second column is its connected node.\n",
    "\n",
    "Information of nodes and their correponding labels is saved as `group-edges.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_data = pd.read_csv(\"full_df.csv\")\n",
    "unique_IDs = np.unique(prepro_data[\"ID\"])\n",
    "new_ids = []\n",
    "for ind in unique_IDs: \n",
    "    if (prepro_data[prepro_data['ID'] == ind].shape[0]) == 2:\n",
    "        new_ids.append(ind)\n",
    "prepro_data = prepro_data[prepro_data.ID.isin(new_ids)]\n",
    "prepro_data = prepro_data.drop_duplicates(subset = [\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_data[\"node_ID\"] = [i for i in range(1,3035, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data = prepro_data[[\"ID\", \"node_ID\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge.csv Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_edges = []\n",
    "edges = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_data = prepro_data[prepro_data[\"Patient Sex\"] == \"Female\"]\n",
    "male_data = prepro_data[prepro_data[\"Patient Sex\"] == \"Male\"]\n",
    "female_patient_age = np.array(female_data[\"Patient Age\"])\n",
    "male_patient_age = np.array(male_data[\"Patient Age\"])\n",
    "female_IDs = np.array(female_data[\"ID\"])\n",
    "male_IDs = np.array(male_data[\"ID\"])\n",
    "\n",
    "for age in np.unique(female_patient_age):\n",
    "    indices = np.argwhere((female_patient_age - age) < 5).reshape(-1)\n",
    "    same_age_ids = female_IDs[indices]\n",
    "    for subset in itertools.combinations(same_age_ids, 2):\n",
    "        list_edges.append(subset)   \n",
    "        edges.append(str(subset[0]) + \",\" + str(subset[1]) + \"\\n\")\n",
    "\n",
    "for age in np.unique(male_patient_age):\n",
    "    indices = np.argwhere((male_patient_age - age) < 5).reshape(-1)\n",
    "    same_age_ids = male_IDs[indices]\n",
    "    for subset in itertools.combinations(same_age_ids, 2):\n",
    "        list_edges.append(subset)   \n",
    "        edges.append(str(subset[0]) + \",\" + str(subset[1]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_file = \"ODIR/edges.csv\"\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(edges_file)\n",
    "f = open(edges_file, \"r+\")\n",
    "f.writelines(edges)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group-edges.csv Construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = prepro_data[[\"ID\", \"node_ID\", \"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = np.array(label_data[\"node_ID\"])\n",
    "labels = []\n",
    "list_edges = []\n",
    "edges = []\n",
    "for ID in IDs:\n",
    "    line = label_data[label_data[\"node_ID\"] == ID]\n",
    "    l = [int(line[\"N\"]),int(line[\"D\"]),int(line[\"G\"]),int(line[\"C\"]),int(line[\"A\"]),int(line[\"H\"]),int(line[\"M\"]),int(line[\"O\"])]\n",
    "    labels.append(l)\n",
    "    indices = np.argwhere(np.array(l) == 1).reshape(-1)\n",
    "    for i in indices:\n",
    "        list_edges.append([ID,i+1])\n",
    "        edges.append(str(ID) + \",\" + str(i+1) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label_file = \"ODIR/group-edges.csv\"\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(node_label_file)\n",
    "f = open(node_label_file, \"r+\")\n",
    "f.writelines(edges)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Function and custom made functions\n",
    "\n",
    "`normalize` : used in the normalization step for the adjacency matrix\n",
    "\n",
    "`threshold` : used for defining the sigmoid output to 0 or 1\n",
    "\n",
    "`accuracy_sample_class` : accuracy for samples having multilabels\n",
    "\n",
    "`micor_F1` : built based on the equations provided in the paper\n",
    "\n",
    "`load_data` : creating the input feature matrices, adjacency matrices and embeddings as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "m = 8\n",
    "# number of patients\n",
    "n = 3034\n",
    "# number of features\n",
    "nfeatures = 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_F1(output, labels):\n",
    "    TP = np.array([])\n",
    "    FN = np.array([])\n",
    "    FP = np.array([])\n",
    "    N = labels.shape[0]\n",
    "    L = labels.shape[1]\n",
    "    for l in range(L):\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        ol = output[:,l]\n",
    "        ll = labels[:,l]\n",
    "        for i in range(N):\n",
    "            if ll[i] == 1 and ol[i] == 1:\n",
    "                tp +=1\n",
    "            if ol[i] == 0 and ll[i] == 1:\n",
    "                fn +=1\n",
    "            if ol[i] == 1 and ll[i] == 0:\n",
    "                fp +=1\n",
    "        TP = np.append(TP, tp)\n",
    "        FN = np.append(FN, fn)\n",
    "        FP = np.append(FP, fp)\n",
    "    return np.sum(2*TP)/np.sum(2*TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    #label_occ_file = data_name + \"/label_co-occurences.csv\"\n",
    "    nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "    llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], m))\n",
    "    for l in range(1, m+1, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "            \n",
    "    main_graph = open(edges_file, \"rb\")\n",
    "    G = nx.read_edgelist(main_graph, delimiter = \",\", nodetype = int)\n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes)\n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    A_tilde = normalize(A + 3*sp.eye(A.shape[0]))\n",
    "    \n",
    "    # node-node-label graph \n",
    "    label_nodes = label_raw + unique_nodes.shape[0] \n",
    "    n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(nnlg_file)\n",
    "    f = open(nnlg_file, \"r+\")\n",
    "    file_to_read = open(edges_file, \"r\")\n",
    "    f.writelines(file_to_read.readlines())\n",
    "    a = np.dstack((nodes, label_nodes)).reshape(label_nodes.shape[0],2)\n",
    "    e = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a.astype(str)]\n",
    "    f.writelines(e)\n",
    "    f.close()\n",
    "    \n",
    "    nnl_graph = nx.read_edgelist(nnlg_file, delimiter = \",\", nodetype = int)\n",
    "    E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    E = normalize(E + 3*sp.eye(E.shape[0]))\n",
    "    E_tilde = E[:len(unique_nodes)]\n",
    "    \n",
    "    # Label-label-node graph \n",
    "    edges = []\n",
    "    list_edges = []\n",
    "    for k in range(labels.shape[0]):\n",
    "        indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "        if indices.shape[0]>1:\n",
    "            for subset in itertools.combinations(indices, 2): \n",
    "                if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                    list_edges.append([subset[0], subset[1]+labels.shape[0]])# check if the common nodes should be before or after the label nodes\n",
    "                    edges.append(str(subset[0]+1) + \",\" + str(subset[1] +1) + \"\\n\")\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(llng_file)\n",
    "    label_file = open(llng_file, \"r+\")\n",
    "    label_file.writelines(edges)\n",
    "    label_file.close()\n",
    "    \n",
    "    unique_label_ID = np.arange(1,m+1)\n",
    "    label_file = open(llng_file, \"rb\")\n",
    "    label_graph = nx.read_edgelist(label_file, delimiter = \",\", nodetype = int)\n",
    "    C = nx.adjacency_matrix(label_graph, nodelist = unique_label_ID)\n",
    "    label_file.close()\n",
    "    # Normalize the co-occurence matrix with the added self loops\n",
    "    C_tilde = normalize(C + 2*sp.eye(C.shape[0]))\n",
    "    Y = np.random.rand(m,nfeatures) #instead of X[:m]\n",
    "    Y_embedding = Y[:,:400]\n",
    "    \n",
    "    labels_ind = label_raw \n",
    "    nodes_ind = nodes + m\n",
    "    a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "    e_1 = [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]\n",
    "    file = open(llng_file, \"r+\")\n",
    "    file.writelines(e_1)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "    f_1 = open(llng_file, \"rb\")\n",
    "    l_l_n_nodes = np.concatenate((np.unique(nodes_ind),np.unique(labels_ind)))\n",
    "    lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "    F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "    F = sp.coo_matrix(F.todense())\n",
    "    f_1.close()\n",
    "    \n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    F = normalize(F + 2*sp.eye(F.shape[0]))\n",
    "    F_tilde = F[:m]\n",
    "    \n",
    "    indices = np.arange(A.shape[0]).astype('int32') # should be shuffled\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # idx_train = indices[:A.shape[0] // 3]\n",
    "    # idx_val = indices[A.shape[0] // 3: (2 * A.shape[0]) // 3]\n",
    "    # idx_test = indices[(2 * A.shape[0]) // 3:]\n",
    "    # idx_train = torch.LongTensor(idx_train)\n",
    "    # idx_val = torch.LongTensor(idx_val)\n",
    "    # idx_test = torch.LongTensor(idx_test)\n",
    "    C_tilde = torch.FloatTensor(np.array(C_tilde.todense()))\n",
    "    E_tilde = torch.FloatTensor(np.array(E_tilde.todense()))\n",
    "    F_tilde = torch.FloatTensor(np.array(F_tilde.todense()))\n",
    "    A_tilde = torch.FloatTensor(np.array(A_tilde.todense()))\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    Y = torch.FloatTensor(Y)\n",
    "    #X_embedding = torch.FloatTensor(X_embedding)\n",
    "    Y_embedding = torch.FloatTensor(Y_embedding)\n",
    "    \n",
    "    return Y, Y_embedding, F_tilde, E_tilde, C_tilde, A_tilde, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Y_embedding, F_tilde, E_tilde, C_tilde, A_tilde, labels = load_data(\"ODIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the X feature matrix from saved `X.csv` file, the file does not exist in the current folder due to large dataset,\n",
    "\n",
    "create the file by the jupyter notebook provided in README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is for importing the X feature matrix \n",
    "# which is extracted and saved from the fundus images from ODIR dataset\n",
    "X = pd.read_csv(\"X.csv\", sep=\",\", header = None)\n",
    "X = torch.FloatTensor(X.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read indices\n",
    "\n",
    "Read the patient indices for `train`, `validation` and `test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = pd.read_csv(\"annotations_file_train.csv\", sep=\",\", header = None)\n",
    "val_indices = pd.read_csv(\"annotations_file_val.csv\", sep=\",\", header = None)\n",
    "test_indices = pd.read_csv(\"annotations_file_test.csv\", sep=\",\", header = None)\n",
    "\n",
    "\n",
    "map_table = mapping_data['ID'].to_numpy()\n",
    "node_table = mapping_data['node_ID'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices(csv_indices):\n",
    "    \n",
    "    Left_Fundus = csv_indices[0]\n",
    "    selected_indices = []\n",
    "    for id in range(1,len(Left_Fundus), 1):\n",
    "        index = node_table[int(np.argwhere(map_table == int(Left_Fundus[id].split('_')[0])).reshape(-1))]\n",
    "        selected_indices.append(index)\n",
    "\n",
    "    return np.array(selected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = torch.LongTensor(load_indices(train_indices)-1)\n",
    "idx_val = torch.LongTensor(load_indices(val_indices)-1)\n",
    "idx_test = torch.LongTensor(load_indices(test_indices)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The cell below contains the models importing from `models1.py`, if the **Jupyter Notebook IDE** is used,\n",
    "\n",
    "change `from models1 import High_Layer, Low_Layer, GCN` \n",
    "\n",
    "to `from MLMI_MLGCN_project.models1 import High_Layer, Low_Layer, GCN`\n",
    "\n",
    "For training the two layers GCN, the procedure contains two parts:\n",
    "\n",
    "`train_embedding` : we first train the meaningful embedding to give it as the input to high_layer, # training epochs = 100.\n",
    "\n",
    "`train_sep` : training high_layer and low_layer one by one, start with high_layer, each layer will be train for 50 epochs as suggested in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from models1 import High_Layer, Low_Layer, GCN, GCN1\n",
    "\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=400,\n",
    "                    help='Number of hidden units.')                   \n",
    "parser.add_argument('--hidden_ll', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--hidden_hl', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_GCN = GCN(nfeat = X.shape[1],\n",
    "                nhid = args.hidden,\n",
    "                nclass = m, \n",
    "                dropout = args.dropout)\n",
    "# Think about removing the dropout or changing the probability of dropping a neuron\n",
    "optimizer_emb = optim.Adam(simple_GCN.parameters(),lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_list = []\n",
    "loss_val_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(epoch):\n",
    "\n",
    "    global loss_train_list\n",
    "    global loss_val_list\n",
    "\n",
    "\n",
    "    simple_GCN.train()\n",
    "    optimizer_emb.zero_grad()\n",
    "    output, X_embedding = simple_GCN(X, A_tilde)\n",
    "    loss_train = np.sum([F.binary_cross_entropy_with_logits(output[:][:,i], labels[:][:,i]) for i in range(m)])\n",
    "    micro_F1_train = micro_F1(threshold(output.detach().numpy()[:]), labels.detach().numpy()[:])\n",
    "    # macro_F1_train = macro_F1(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_emb.step()\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'micro_F1_train: {:.4f}'.format(micro_F1_train.item())\n",
    "        #   'loss_val: {:.4f}'.format(loss_val.item())\n",
    "          )\n",
    "\n",
    "    loss_train_list.append(loss_train.item())\n",
    "    # loss_val_list.append(loss_val.item())\n",
    "    \n",
    "\n",
    "    return X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 7.4924 micro_F1_train: 0.2473\n",
      "Epoch: 0002 loss_train: 7.4859 micro_F1_train: 0.2477\n",
      "Epoch: 0003 loss_train: 7.4713 micro_F1_train: 0.2502\n",
      "Epoch: 0004 loss_train: 7.4754 micro_F1_train: 0.2494\n",
      "Epoch: 0005 loss_train: 7.4821 micro_F1_train: 0.2500\n",
      "Epoch: 0006 loss_train: 7.4917 micro_F1_train: 0.2468\n",
      "Epoch: 0007 loss_train: 7.4809 micro_F1_train: 0.2491\n",
      "Epoch: 0008 loss_train: 7.4794 micro_F1_train: 0.2502\n",
      "Epoch: 0009 loss_train: 7.4707 micro_F1_train: 0.2501\n",
      "Epoch: 0010 loss_train: 7.4780 micro_F1_train: 0.2506\n",
      "Epoch: 0011 loss_train: 7.4873 micro_F1_train: 0.2465\n",
      "Epoch: 0012 loss_train: 7.4815 micro_F1_train: 0.2491\n",
      "Epoch: 0013 loss_train: 7.4877 micro_F1_train: 0.2461\n",
      "Epoch: 0014 loss_train: 7.4729 micro_F1_train: 0.2497\n",
      "Epoch: 0015 loss_train: 7.4775 micro_F1_train: 0.2489\n",
      "Epoch: 0016 loss_train: 7.4833 micro_F1_train: 0.2508\n",
      "Epoch: 0017 loss_train: 7.4883 micro_F1_train: 0.2484\n",
      "Epoch: 0018 loss_train: 7.4749 micro_F1_train: 0.2502\n",
      "Epoch: 0019 loss_train: 7.4745 micro_F1_train: 0.2484\n",
      "Epoch: 0020 loss_train: 7.4759 micro_F1_train: 0.2493\n",
      "Epoch: 0021 loss_train: 7.4792 micro_F1_train: 0.2487\n",
      "Epoch: 0022 loss_train: 7.4734 micro_F1_train: 0.2482\n",
      "Epoch: 0023 loss_train: 7.4762 micro_F1_train: 0.2487\n",
      "Epoch: 0024 loss_train: 7.4892 micro_F1_train: 0.2491\n",
      "Epoch: 0025 loss_train: 7.4804 micro_F1_train: 0.2483\n",
      "Epoch: 0026 loss_train: 7.4788 micro_F1_train: 0.2489\n",
      "Epoch: 0027 loss_train: 7.4759 micro_F1_train: 0.2494\n",
      "Epoch: 0028 loss_train: 7.4808 micro_F1_train: 0.2473\n",
      "Epoch: 0029 loss_train: 7.4763 micro_F1_train: 0.2463\n",
      "Epoch: 0030 loss_train: 7.4786 micro_F1_train: 0.2477\n",
      "Epoch: 0031 loss_train: 7.4817 micro_F1_train: 0.2731\n",
      "Epoch: 0032 loss_train: 7.4784 micro_F1_train: 0.2496\n",
      "Epoch: 0033 loss_train: 7.4847 micro_F1_train: 0.2480\n",
      "Epoch: 0034 loss_train: 7.4765 micro_F1_train: 0.2488\n",
      "Epoch: 0035 loss_train: 7.4814 micro_F1_train: 0.2504\n",
      "Epoch: 0036 loss_train: 7.4783 micro_F1_train: 0.2482\n",
      "Epoch: 0037 loss_train: 7.4863 micro_F1_train: 0.2807\n",
      "Epoch: 0038 loss_train: 7.4914 micro_F1_train: 0.2803\n",
      "Epoch: 0039 loss_train: 7.4771 micro_F1_train: 0.2505\n",
      "Epoch: 0040 loss_train: 7.4893 micro_F1_train: 0.2453\n",
      "Epoch: 0041 loss_train: 7.4893 micro_F1_train: 0.2455\n",
      "Epoch: 0042 loss_train: 7.4895 micro_F1_train: 0.2481\n",
      "Epoch: 0043 loss_train: 7.4770 micro_F1_train: 0.2492\n",
      "Epoch: 0044 loss_train: 7.4768 micro_F1_train: 0.2498\n",
      "Epoch: 0045 loss_train: 7.4843 micro_F1_train: 0.2497\n",
      "Epoch: 0046 loss_train: 7.4778 micro_F1_train: 0.2490\n",
      "Epoch: 0047 loss_train: 7.4879 micro_F1_train: 0.2462\n",
      "Epoch: 0048 loss_train: 7.4812 micro_F1_train: 0.2783\n",
      "Epoch: 0049 loss_train: 7.4741 micro_F1_train: 0.2473\n",
      "Epoch: 0050 loss_train: 7.4903 micro_F1_train: 0.2487\n",
      "Epoch: 0051 loss_train: 7.4850 micro_F1_train: 0.2497\n",
      "Epoch: 0052 loss_train: 7.4705 micro_F1_train: 0.2505\n",
      "Epoch: 0053 loss_train: 7.4878 micro_F1_train: 0.2801\n",
      "Epoch: 0054 loss_train: 7.4780 micro_F1_train: 0.2463\n",
      "Epoch: 0055 loss_train: 7.4790 micro_F1_train: 0.2490\n",
      "Epoch: 0056 loss_train: 7.4754 micro_F1_train: 0.2461\n",
      "Epoch: 0057 loss_train: 7.4932 micro_F1_train: 0.2462\n",
      "Epoch: 0058 loss_train: 7.4753 micro_F1_train: 0.2502\n",
      "Epoch: 0059 loss_train: 7.4849 micro_F1_train: 0.2616\n",
      "Epoch: 0060 loss_train: 7.4717 micro_F1_train: 0.2484\n",
      "Epoch: 0061 loss_train: 7.4728 micro_F1_train: 0.2498\n",
      "Epoch: 0062 loss_train: 7.4813 micro_F1_train: 0.2473\n",
      "Epoch: 0063 loss_train: 7.4787 micro_F1_train: 0.2500\n",
      "Epoch: 0064 loss_train: 7.4829 micro_F1_train: 0.2793\n",
      "Epoch: 0065 loss_train: 7.4801 micro_F1_train: 0.2510\n",
      "Epoch: 0066 loss_train: 7.4685 micro_F1_train: 0.2488\n",
      "Epoch: 0067 loss_train: 7.4824 micro_F1_train: 0.2482\n",
      "Epoch: 0068 loss_train: 7.4869 micro_F1_train: 0.2458\n",
      "Epoch: 0069 loss_train: 7.4698 micro_F1_train: 0.2501\n",
      "Epoch: 0070 loss_train: 7.4809 micro_F1_train: 0.2497\n",
      "Epoch: 0071 loss_train: 7.4762 micro_F1_train: 0.2472\n",
      "Epoch: 0072 loss_train: 7.4897 micro_F1_train: 0.2482\n",
      "Epoch: 0073 loss_train: 7.4701 micro_F1_train: 0.2520\n",
      "Epoch: 0074 loss_train: 7.4832 micro_F1_train: 0.2798\n",
      "Epoch: 0075 loss_train: 7.4709 micro_F1_train: 0.2486\n",
      "Epoch: 0076 loss_train: 7.4821 micro_F1_train: 0.2493\n",
      "Epoch: 0077 loss_train: 7.4799 micro_F1_train: 0.2489\n",
      "Epoch: 0078 loss_train: 7.4847 micro_F1_train: 0.2482\n",
      "Epoch: 0079 loss_train: 7.4879 micro_F1_train: 0.2497\n",
      "Epoch: 0080 loss_train: 7.4751 micro_F1_train: 0.2824\n",
      "Epoch: 0081 loss_train: 7.4688 micro_F1_train: 0.2492\n",
      "Epoch: 0082 loss_train: 7.4760 micro_F1_train: 0.2481\n",
      "Epoch: 0083 loss_train: 7.4811 micro_F1_train: 0.2486\n",
      "Epoch: 0084 loss_train: 7.4852 micro_F1_train: 0.2476\n",
      "Epoch: 0085 loss_train: 7.4800 micro_F1_train: 0.2482\n",
      "Epoch: 0086 loss_train: 7.4909 micro_F1_train: 0.2732\n",
      "Epoch: 0087 loss_train: 7.4774 micro_F1_train: 0.2488\n",
      "Epoch: 0088 loss_train: 7.4848 micro_F1_train: 0.2801\n",
      "Epoch: 0089 loss_train: 7.4843 micro_F1_train: 0.2488\n",
      "Epoch: 0090 loss_train: 7.4919 micro_F1_train: 0.2486\n",
      "Epoch: 0091 loss_train: 7.4944 micro_F1_train: 0.2810\n",
      "Epoch: 0092 loss_train: 7.4903 micro_F1_train: 0.2810\n",
      "Epoch: 0093 loss_train: 7.4884 micro_F1_train: 0.2523\n",
      "Epoch: 0094 loss_train: 7.4860 micro_F1_train: 0.2478\n",
      "Epoch: 0095 loss_train: 7.4685 micro_F1_train: 0.2495\n",
      "Epoch: 0096 loss_train: 7.4757 micro_F1_train: 0.2496\n",
      "Epoch: 0097 loss_train: 7.4879 micro_F1_train: 0.2457\n",
      "Epoch: 0098 loss_train: 7.4765 micro_F1_train: 0.2492\n",
      "Epoch: 0099 loss_train: 7.4833 micro_F1_train: 0.2484\n",
      "Epoch: 0100 loss_train: 7.4900 micro_F1_train: 0.2473\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    X_ = train_embedding(epoch)\n",
    "\n",
    "X_embedding = X_.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models (high layer and low layer) and the optimizers for each on of them\n",
    "low_layer = Low_Layer(nfeat = nfeatures,\n",
    "                      nhid_lowlayer = args.hidden_ll,\n",
    "                      nhid_highlayer = args.hidden_hl,\n",
    "                      nclass = C_tilde.shape[0],\n",
    "                      dropout = args.dropout)\n",
    "optimizer_lowlayer = optim.Adam(low_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "high_layer = High_Layer(nfeat = nfeatures,\n",
    "                        nhid_lowlayer = args.hidden_ll,\n",
    "                        nhid_highlayer = args.hidden_hl,\n",
    "                        nclass = C_tilde.shape[0],\n",
    "                        dropout = args.dropout)\n",
    "optimizer_highlayer = optim.Adam(high_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = torch.LongTensor(np.arange(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X_embedding.clone()\n",
    "Y_ = Y_embedding.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_list = []\n",
    "micro_F1_train_list = []\n",
    "loss_val_list = []\n",
    "micro_F1_val_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training two layers sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_low(epoch):\n",
    "      # training\n",
    "      global loss_train_list \n",
    "      global micro_F1_train_list \n",
    "      global loss_val_list\n",
    "      global micro_F1_val_list\n",
    "      low_layer.train()\n",
    "      optimizer_lowlayer.zero_grad()\n",
    "      output_ll, X_embedding_ = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "    \n",
    "      X_embedding_ = X_embedding_.detach()\n",
    "    \n",
    "      loss_train_ll = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "      micro_F1_train = micro_F1(threshold(output_ll.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "      loss_train_ll.backward()\n",
    "      optimizer_lowlayer.step()\n",
    "      loss_train_list.append(loss_train_ll.item())\n",
    "      micro_F1_train_list.append(micro_F1_train.item())\n",
    "    \n",
    "      # validation part\n",
    "      low_layer.eval()\n",
    "    #   output_ll, X_embedding_val = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "      loss_val = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_val][:,i], labels[idx_val][:,i]) for i in range(m)])\n",
    "      micro_F1_val = micro_F1(threshold(output_ll.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "      loss_val_list.append(loss_val.item())\n",
    "      micro_F1_val_list.append(micro_F1_val.item())\n",
    "\n",
    "      print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_ll.item()),\n",
    "          'micro_F1_train: {:.4f}'.format(micro_F1_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(micro_F1_val.item())\n",
    "          )\n",
    "\n",
    "      return X_embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_high(epoch):\n",
    "    high_layer.train()\n",
    "    optimizer_highlayer.zero_grad()\n",
    "    output_hl, Y_embedding_ = high_layer(X_, Y, F_tilde, C_tilde)\n",
    "    Y_embedding_ = Y_embedding_.detach()\n",
    "    loss_train_hl = F.nll_loss(output_hl, truth)\n",
    "    loss_train_hl.backward()\n",
    "    optimizer_highlayer.step()\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_hl.item()),\n",
    "          #'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          )\n",
    "    \n",
    "    return Y_embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sep(epoch, M, N):\n",
    "    global Y_\n",
    "    global X_\n",
    "\n",
    "    if np.floor(epoch/M) % 2 == 0:\n",
    "        Y_ = train_high(epoch)\n",
    "        \n",
    "    if np.floor(epoch/M) % 2 == 1:\n",
    "        X_ = train_low(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 5.9588\n",
      "Epoch: 0002 loss_train: 25.0866\n",
      "Epoch: 0003 loss_train: 34.8865\n",
      "Epoch: 0004 loss_train: 42.5095\n",
      "Epoch: 0005 loss_train: 48.1995\n",
      "Epoch: 0006 loss_train: 50.8391\n",
      "Epoch: 0007 loss_train: 42.4051\n",
      "Epoch: 0008 loss_train: 31.5058\n",
      "Epoch: 0009 loss_train: 17.2969\n",
      "Epoch: 0010 loss_train: 21.7252\n",
      "Epoch: 0011 loss_train: 11.4794\n",
      "Epoch: 0012 loss_train: 11.2911\n",
      "Epoch: 0013 loss_train: 9.1045\n",
      "Epoch: 0014 loss_train: 9.5138\n",
      "Epoch: 0015 loss_train: 7.8311\n",
      "Epoch: 0016 loss_train: 3.9260\n",
      "Epoch: 0017 loss_train: 2.9100\n",
      "Epoch: 0018 loss_train: 4.0502\n",
      "Epoch: 0019 loss_train: 6.1580\n",
      "Epoch: 0020 loss_train: 4.4861\n",
      "Epoch: 0021 loss_train: 3.8410\n",
      "Epoch: 0022 loss_train: 3.5176\n",
      "Epoch: 0023 loss_train: 4.7687\n",
      "Epoch: 0024 loss_train: 3.7360\n",
      "Epoch: 0025 loss_train: 1.8708\n",
      "Epoch: 0026 loss_train: 5.8525\n",
      "Epoch: 0027 loss_train: 7.0333\n",
      "Epoch: 0028 loss_train: 6.1116\n",
      "Epoch: 0029 loss_train: 4.9359\n",
      "Epoch: 0030 loss_train: 4.3984\n",
      "Epoch: 0031 loss_train: 4.6257\n",
      "Epoch: 0032 loss_train: 5.6285\n",
      "Epoch: 0033 loss_train: 4.8121\n",
      "Epoch: 0034 loss_train: 5.0236\n",
      "Epoch: 0035 loss_train: 7.2388\n",
      "Epoch: 0036 loss_train: 5.5607\n",
      "Epoch: 0037 loss_train: 2.9171\n",
      "Epoch: 0038 loss_train: 2.9624\n",
      "Epoch: 0039 loss_train: 5.0614\n",
      "Epoch: 0040 loss_train: 2.6367\n",
      "Epoch: 0041 loss_train: 5.4893\n",
      "Epoch: 0042 loss_train: 2.7971\n",
      "Epoch: 0043 loss_train: 3.2020\n",
      "Epoch: 0044 loss_train: 3.1445\n",
      "Epoch: 0045 loss_train: 2.6665\n",
      "Epoch: 0046 loss_train: 2.1191\n",
      "Epoch: 0047 loss_train: 2.3233\n",
      "Epoch: 0048 loss_train: 2.9483\n",
      "Epoch: 0049 loss_train: 2.6590\n",
      "Epoch: 0050 loss_train: 2.2477\n",
      "Epoch: 0051 loss_train: 6.7932 micro_F1_train: 0.0655 loss_val: 6.7871 acc_val: 0.0687\n",
      "Epoch: 0052 loss_train: 6.0959 micro_F1_train: 0.0546 loss_val: 6.0979 acc_val: 0.0518\n",
      "Epoch: 0053 loss_train: 5.5485 micro_F1_train: 0.0204 loss_val: 5.5605 acc_val: 0.0055\n",
      "Epoch: 0054 loss_train: 5.5479 micro_F1_train: 0.0224 loss_val: 5.5437 acc_val: 0.0112\n",
      "Epoch: 0055 loss_train: 5.5460 micro_F1_train: 0.0142 loss_val: 5.5458 acc_val: 0.0167\n",
      "Epoch: 0056 loss_train: 5.5441 micro_F1_train: 0.0114 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0057 loss_train: 5.5462 micro_F1_train: 0.0029 loss_val: 5.5430 acc_val: 0.0168\n",
      "Epoch: 0058 loss_train: 5.5461 micro_F1_train: 0.0043 loss_val: 5.5484 acc_val: 0.0000\n",
      "Epoch: 0059 loss_train: 5.5446 micro_F1_train: 0.0064 loss_val: 5.5470 acc_val: 0.0000\n",
      "Epoch: 0060 loss_train: 5.5443 micro_F1_train: 0.0064 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0061 loss_train: 5.5451 micro_F1_train: 0.0057 loss_val: 5.5441 acc_val: 0.0056\n",
      "Epoch: 0062 loss_train: 5.5461 micro_F1_train: 0.0064 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0063 loss_train: 5.5441 micro_F1_train: 0.0107 loss_val: 5.5461 acc_val: 0.0000\n",
      "Epoch: 0064 loss_train: 5.5445 micro_F1_train: 0.0107 loss_val: 5.5440 acc_val: 0.0056\n",
      "Epoch: 0065 loss_train: 5.5443 micro_F1_train: 0.0114 loss_val: 5.5446 acc_val: 0.0112\n",
      "Epoch: 0066 loss_train: 5.5439 micro_F1_train: 0.0142 loss_val: 5.5465 acc_val: 0.0112\n",
      "Epoch: 0067 loss_train: 5.5441 micro_F1_train: 0.0142 loss_val: 5.5500 acc_val: 0.0056\n",
      "Epoch: 0068 loss_train: 5.5426 micro_F1_train: 0.0199 loss_val: 5.5439 acc_val: 0.0056\n",
      "Epoch: 0069 loss_train: 5.5454 micro_F1_train: 0.0135 loss_val: 5.5435 acc_val: 0.0168\n",
      "Epoch: 0070 loss_train: 5.5420 micro_F1_train: 0.0206 loss_val: 5.5445 acc_val: 0.0222\n",
      "Epoch: 0071 loss_train: 5.5443 micro_F1_train: 0.0177 loss_val: 5.5451 acc_val: 0.0276\n",
      "Epoch: 0072 loss_train: 5.5429 micro_F1_train: 0.0219 loss_val: 5.5475 acc_val: 0.0112\n",
      "Epoch: 0073 loss_train: 5.5427 micro_F1_train: 0.0219 loss_val: 5.5423 acc_val: 0.0223\n",
      "Epoch: 0074 loss_train: 5.5454 micro_F1_train: 0.0114 loss_val: 5.5498 acc_val: 0.0056\n",
      "Epoch: 0075 loss_train: 5.5447 micro_F1_train: 0.0142 loss_val: 5.5473 acc_val: 0.0000\n",
      "Epoch: 0076 loss_train: 5.5438 micro_F1_train: 0.0177 loss_val: 5.5427 acc_val: 0.0112\n",
      "Epoch: 0077 loss_train: 5.5431 micro_F1_train: 0.0170 loss_val: 5.5467 acc_val: 0.0112\n",
      "Epoch: 0078 loss_train: 5.5430 micro_F1_train: 0.0205 loss_val: 5.5424 acc_val: 0.0223\n",
      "Epoch: 0079 loss_train: 5.5454 micro_F1_train: 0.0156 loss_val: 5.5407 acc_val: 0.0278\n",
      "Epoch: 0080 loss_train: 5.5434 micro_F1_train: 0.0205 loss_val: 5.5419 acc_val: 0.0385\n",
      "Epoch: 0081 loss_train: 5.5409 micro_F1_train: 0.0275 loss_val: 5.5527 acc_val: 0.0111\n",
      "Epoch: 0082 loss_train: 5.5420 micro_F1_train: 0.0309 loss_val: 5.5342 acc_val: 0.0496\n",
      "Epoch: 0083 loss_train: 5.5407 micro_F1_train: 0.0419 loss_val: 5.5475 acc_val: 0.0167\n",
      "Epoch: 0084 loss_train: 5.5424 micro_F1_train: 0.0458 loss_val: 5.5425 acc_val: 0.0277\n",
      "Epoch: 0085 loss_train: 5.5397 micro_F1_train: 0.0559 loss_val: 5.5418 acc_val: 0.0543\n",
      "Epoch: 0086 loss_train: 5.5430 micro_F1_train: 0.0616 loss_val: 5.5468 acc_val: 0.0539\n",
      "Epoch: 0087 loss_train: 5.5367 micro_F1_train: 0.0958 loss_val: 5.5434 acc_val: 0.0942\n",
      "Epoch: 0088 loss_train: 5.5401 micro_F1_train: 0.1427 loss_val: 5.5627 acc_val: 0.1156\n",
      "Epoch: 0089 loss_train: 5.5370 micro_F1_train: 0.0865 loss_val: 5.5468 acc_val: 0.0538\n",
      "Epoch: 0090 loss_train: 5.5414 micro_F1_train: 0.0323 loss_val: 5.5430 acc_val: 0.0331\n",
      "Epoch: 0091 loss_train: 5.5444 micro_F1_train: 0.0093 loss_val: 5.5448 acc_val: 0.0112\n",
      "Epoch: 0092 loss_train: 5.5443 micro_F1_train: 0.0050 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0093 loss_train: 5.5450 micro_F1_train: 0.0007 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0094 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0095 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0096 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0097 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0098 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0099 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0100 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0101 loss_train: 170.7292\n",
      "Epoch: 0102 loss_train: 81.3585\n",
      "Epoch: 0103 loss_train: 97.1412\n",
      "Epoch: 0104 loss_train: 104.5407\n",
      "Epoch: 0105 loss_train: 143.9285\n",
      "Epoch: 0106 loss_train: 142.2037\n",
      "Epoch: 0107 loss_train: 82.3702\n",
      "Epoch: 0108 loss_train: 136.3407\n",
      "Epoch: 0109 loss_train: 88.8147\n",
      "Epoch: 0110 loss_train: 91.4926\n",
      "Epoch: 0111 loss_train: 58.8746\n",
      "Epoch: 0112 loss_train: 61.8573\n",
      "Epoch: 0113 loss_train: 61.1856\n",
      "Epoch: 0114 loss_train: 149.1941\n",
      "Epoch: 0115 loss_train: 81.2021\n",
      "Epoch: 0116 loss_train: 80.9855\n",
      "Epoch: 0117 loss_train: 69.2813\n",
      "Epoch: 0118 loss_train: 68.0750\n",
      "Epoch: 0119 loss_train: 79.5700\n",
      "Epoch: 0120 loss_train: 56.1020\n",
      "Epoch: 0121 loss_train: 82.4479\n",
      "Epoch: 0122 loss_train: 64.6823\n",
      "Epoch: 0123 loss_train: 86.9078\n",
      "Epoch: 0124 loss_train: 75.1761\n",
      "Epoch: 0125 loss_train: 52.1738\n",
      "Epoch: 0126 loss_train: 48.2719\n",
      "Epoch: 0127 loss_train: 32.4892\n",
      "Epoch: 0128 loss_train: 34.1450\n",
      "Epoch: 0129 loss_train: 16.0231\n",
      "Epoch: 0130 loss_train: 12.8690\n",
      "Epoch: 0131 loss_train: 15.5276\n",
      "Epoch: 0132 loss_train: 11.6092\n",
      "Epoch: 0133 loss_train: 20.2344\n",
      "Epoch: 0134 loss_train: 6.4648\n",
      "Epoch: 0135 loss_train: 3.4590\n",
      "Epoch: 0136 loss_train: 3.3023\n",
      "Epoch: 0137 loss_train: 2.4832\n",
      "Epoch: 0138 loss_train: 2.1072\n",
      "Epoch: 0139 loss_train: 2.1072\n",
      "Epoch: 0140 loss_train: 2.1071\n",
      "Epoch: 0141 loss_train: 2.1071\n",
      "Epoch: 0142 loss_train: 2.1070\n",
      "Epoch: 0143 loss_train: 2.1070\n",
      "Epoch: 0144 loss_train: 2.1070\n",
      "Epoch: 0145 loss_train: 2.1069\n",
      "Epoch: 0146 loss_train: 2.1069\n",
      "Epoch: 0147 loss_train: 2.1068\n",
      "Epoch: 0148 loss_train: 2.1068\n",
      "Epoch: 0149 loss_train: 2.1068\n",
      "Epoch: 0150 loss_train: 2.1067\n",
      "Epoch: 0151 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0152 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0153 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0154 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0155 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0156 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0157 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0158 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0159 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0160 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0161 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0162 loss_train: 5.5455 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0163 loss_train: 5.5450 micro_F1_train: 0.0007 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0164 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0165 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0166 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0167 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0168 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0169 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0170 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0171 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0172 loss_train: 5.5451 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0173 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0174 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0175 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0176 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0177 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0178 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0179 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0180 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0181 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0182 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0183 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0184 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0185 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0186 loss_train: 5.5453 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0187 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0188 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0189 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0190 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0191 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0192 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0193 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0194 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0195 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0196 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0197 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0198 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0199 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0200 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0201 loss_train: 2.1067\n",
      "Epoch: 0202 loss_train: 2.1066\n",
      "Epoch: 0203 loss_train: 2.1066\n",
      "Epoch: 0204 loss_train: 2.1066\n",
      "Epoch: 0205 loss_train: 2.1065\n",
      "Epoch: 0206 loss_train: 2.1065\n",
      "Epoch: 0207 loss_train: 2.1065\n",
      "Epoch: 0208 loss_train: 2.1064\n",
      "Epoch: 0209 loss_train: 2.1064\n",
      "Epoch: 0210 loss_train: 2.1064\n",
      "Epoch: 0211 loss_train: 2.1063\n",
      "Epoch: 0212 loss_train: 2.1063\n",
      "Epoch: 0213 loss_train: 2.1062\n",
      "Epoch: 0214 loss_train: 2.1062\n",
      "Epoch: 0215 loss_train: 2.1062\n",
      "Epoch: 0216 loss_train: 2.1061\n",
      "Epoch: 0217 loss_train: 2.1061\n",
      "Epoch: 0218 loss_train: 2.1061\n",
      "Epoch: 0219 loss_train: 2.1060\n",
      "Epoch: 0220 loss_train: 2.1060\n",
      "Epoch: 0221 loss_train: 2.1060\n",
      "Epoch: 0222 loss_train: 2.1059\n",
      "Epoch: 0223 loss_train: 2.1059\n",
      "Epoch: 0224 loss_train: 2.1059\n",
      "Epoch: 0225 loss_train: 2.1058\n",
      "Epoch: 0226 loss_train: 2.1058\n",
      "Epoch: 0227 loss_train: 2.1058\n",
      "Epoch: 0228 loss_train: 2.1057\n",
      "Epoch: 0229 loss_train: 2.1057\n",
      "Epoch: 0230 loss_train: 2.1057\n",
      "Epoch: 0231 loss_train: 2.1056\n",
      "Epoch: 0232 loss_train: 2.1056\n",
      "Epoch: 0233 loss_train: 2.1055\n",
      "Epoch: 0234 loss_train: 2.1055\n",
      "Epoch: 0235 loss_train: 2.1055\n",
      "Epoch: 0236 loss_train: 2.1054\n",
      "Epoch: 0237 loss_train: 2.1054\n",
      "Epoch: 0238 loss_train: 2.1054\n",
      "Epoch: 0239 loss_train: 2.1053\n",
      "Epoch: 0240 loss_train: 2.1053\n",
      "Epoch: 0241 loss_train: 2.1053\n",
      "Epoch: 0242 loss_train: 2.1052\n",
      "Epoch: 0243 loss_train: 2.1052\n",
      "Epoch: 0244 loss_train: 2.1052\n",
      "Epoch: 0245 loss_train: 2.1051\n",
      "Epoch: 0246 loss_train: 2.1051\n",
      "Epoch: 0247 loss_train: 2.1050\n",
      "Epoch: 0248 loss_train: 2.1050\n",
      "Epoch: 0249 loss_train: 2.1050\n",
      "Epoch: 0250 loss_train: 2.1049\n",
      "Epoch: 0251 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0252 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0253 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0254 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0255 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0256 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0257 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0258 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0259 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0260 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0261 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0262 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0263 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0264 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0265 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0266 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0267 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0268 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0269 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0270 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0271 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0272 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0273 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0274 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0275 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0276 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0277 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0278 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0279 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0280 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0281 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0282 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0283 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0284 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0285 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0286 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0287 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0288 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0289 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0290 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0291 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0292 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0293 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0294 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0295 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0296 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0297 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0298 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0299 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n",
      "Epoch: 0300 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 5.5452 acc_val: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# print loss for low layer\n",
    "t_total = time.time()\n",
    "for epoch in range(300):\n",
    "    train_sep(epoch, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same training procedure as literature\n",
    "\n",
    "This part is for demostrating the training procedure proposed in the literature, instead for training high_layer and low_layer one after one, two models are trained at the same time, the update of X_embedding and Y_embedding happens every 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_together(epoch, M, N):# Put M and N in args \n",
    "    global loss_train_list \n",
    "    global micro_F1_train_list \n",
    "    global loss_val_list\n",
    "    global micro_F1_val_list\n",
    "\n",
    "    global X_\n",
    "    global Y_ \n",
    "    t = time.time()\n",
    "    #train the two layers\n",
    "    low_layer.train()\n",
    "    optimizer_lowlayer.zero_grad()\n",
    "    high_layer.train()\n",
    "    optimizer_highlayer.zero_grad()\n",
    "    #The output of the two layers\n",
    "    output_hl, Y_embedding_ = high_layer(X_, Y, F_tilde, C_tilde)\n",
    "    output_ll, X_embedding_ = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "    \n",
    "    #The train losses\n",
    "    loss_train_hl = F.nll_loss(output_hl, truth)\n",
    "    loss_train_ll = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "    \n",
    "    Y_embedding_ = Y_embedding_.detach()\n",
    "    X_embedding_ = X_embedding_.detach()\n",
    "    \n",
    "    micro_F1_train = micro_F1(threshold(output_ll.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    if epoch % M == 0:\n",
    "        X_ = X_embedding_\n",
    "    if epoch % N == 0:\n",
    "        Y_ = Y_embedding_\n",
    "        \n",
    "    \n",
    "    loss_train_ll.backward(retain_graph=True)\n",
    "    loss_train_hl.backward(retain_graph=True)\n",
    "    \n",
    "    optimizer_lowlayer.step()\n",
    "    optimizer_highlayer.step()\n",
    "    \n",
    "    low_layer.eval()\n",
    "    loss_val = (1/m)*np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_val][:,i], labels.detach()[idx_val][:,i]) for i in range(m)])\n",
    "    micro_F1_val = micro_F1(threshold(output_ll.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "\n",
    "    loss_train_list.append(loss_train_ll.item())\n",
    "    loss_val_list.append(loss_val.item())\n",
    "    micro_F1_train_list.append(micro_F1_train.item())\n",
    "    micro_F1_val_list.append(micro_F1_val.item())\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_ll.item()),\n",
    "          'micro_F1_train: {:.4f}'.format(micro_F1_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'micro_F1_val: {:.4f}'.format(micro_F1_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 6.6780 micro_F1_train: 0.2331 loss_val: 0.8411 micro_F1_val: 0.2013 time: 4.5340s\n",
      "Epoch: 0002 loss_train: 5.8533 micro_F1_train: 0.3082 loss_val: 0.7356 micro_F1_val: 0.2811 time: 3.8598s\n",
      "Epoch: 0003 loss_train: 5.8195 micro_F1_train: 0.3107 loss_val: 0.7317 micro_F1_val: 0.2783 time: 4.6415s\n",
      "Epoch: 0004 loss_train: 5.8155 micro_F1_train: 0.3042 loss_val: 0.7304 micro_F1_val: 0.2760 time: 5.3963s\n",
      "Epoch: 0005 loss_train: 5.8103 micro_F1_train: 0.2935 loss_val: 0.7287 micro_F1_val: 0.2667 time: 5.0395s\n",
      "Epoch: 0006 loss_train: 5.7956 micro_F1_train: 0.2620 loss_val: 0.7279 micro_F1_val: 0.2422 time: 3.0548s\n",
      "Epoch: 0007 loss_train: 5.5407 micro_F1_train: 0.0836 loss_val: 0.6936 micro_F1_val: 0.1031 time: 3.9098s\n",
      "Epoch: 0008 loss_train: 5.5377 micro_F1_train: 0.0614 loss_val: 0.6930 micro_F1_val: 0.0437 time: 4.4231s\n",
      "Epoch: 0009 loss_train: 5.5403 micro_F1_train: 0.0358 loss_val: 0.6929 micro_F1_val: 0.0168 time: 4.5912s\n",
      "Epoch: 0010 loss_train: 5.5422 micro_F1_train: 0.0254 loss_val: 0.6924 micro_F1_val: 0.0492 time: 4.7020s\n",
      "Epoch: 0011 loss_train: 5.5444 micro_F1_train: 0.0184 loss_val: 0.6933 micro_F1_val: 0.0112 time: 39.7172s\n",
      "Epoch: 0012 loss_train: 5.5444 micro_F1_train: 0.0093 loss_val: 0.6929 micro_F1_val: 0.0168 time: 11.2773s\n",
      "Epoch: 0013 loss_train: 5.5446 micro_F1_train: 0.0093 loss_val: 0.6928 micro_F1_val: 0.0112 time: 3.8048s\n",
      "Epoch: 0014 loss_train: 5.5444 micro_F1_train: 0.0100 loss_val: 0.6932 micro_F1_val: 0.0056 time: 3.4797s\n",
      "Epoch: 0015 loss_train: 5.5438 micro_F1_train: 0.0093 loss_val: 0.6931 micro_F1_val: 0.0056 time: 3.4082s\n",
      "Epoch: 0016 loss_train: 5.5444 micro_F1_train: 0.0057 loss_val: 0.6932 micro_F1_val: 0.0167 time: 3.5838s\n",
      "Epoch: 0017 loss_train: 5.5435 micro_F1_train: 0.0100 loss_val: 0.6933 micro_F1_val: 0.0112 time: 3.7191s\n",
      "Epoch: 0018 loss_train: 5.5440 micro_F1_train: 0.0079 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.2040s\n",
      "Epoch: 0019 loss_train: 5.5438 micro_F1_train: 0.0093 loss_val: 0.6928 micro_F1_val: 0.0112 time: 3.2208s\n",
      "Epoch: 0020 loss_train: 5.5447 micro_F1_train: 0.0050 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.3733s\n",
      "Epoch: 0021 loss_train: 5.5445 micro_F1_train: 0.0079 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.3111s\n",
      "Epoch: 0022 loss_train: 5.5440 micro_F1_train: 0.0057 loss_val: 0.6930 micro_F1_val: 0.0056 time: 3.6770s\n",
      "Epoch: 0023 loss_train: 5.5436 micro_F1_train: 0.0093 loss_val: 0.6929 micro_F1_val: 0.0168 time: 3.2978s\n",
      "Epoch: 0024 loss_train: 5.5446 micro_F1_train: 0.0086 loss_val: 0.6930 micro_F1_val: 0.0056 time: 3.3023s\n",
      "Epoch: 0025 loss_train: 5.5437 micro_F1_train: 0.0114 loss_val: 0.6933 micro_F1_val: 0.0112 time: 3.4662s\n",
      "Epoch: 0026 loss_train: 5.5425 micro_F1_train: 0.0150 loss_val: 0.6928 micro_F1_val: 0.0223 time: 3.4258s\n",
      "Epoch: 0027 loss_train: 5.5447 micro_F1_train: 0.0057 loss_val: 0.6924 micro_F1_val: 0.0279 time: 3.3524s\n",
      "Epoch: 0028 loss_train: 5.5417 micro_F1_train: 0.0178 loss_val: 0.6931 micro_F1_val: 0.0056 time: 3.1314s\n",
      "Epoch: 0029 loss_train: 5.5421 micro_F1_train: 0.0213 loss_val: 0.6933 micro_F1_val: 0.0112 time: 3.3095s\n",
      "Epoch: 0030 loss_train: 5.5409 micro_F1_train: 0.0234 loss_val: 0.6927 micro_F1_val: 0.0168 time: 3.5943s\n",
      "Epoch: 0031 loss_train: 5.5399 micro_F1_train: 0.0303 loss_val: 0.6920 micro_F1_val: 0.0388 time: 6.1864s\n",
      "Epoch: 0032 loss_train: 5.5395 micro_F1_train: 0.0372 loss_val: 0.6924 micro_F1_val: 0.0332 time: 37.9628s\n",
      "Epoch: 0033 loss_train: 5.5389 micro_F1_train: 0.0460 loss_val: 0.6924 micro_F1_val: 0.0440 time: 37.6343s\n",
      "Epoch: 0034 loss_train: 5.5387 micro_F1_train: 0.0560 loss_val: 0.6927 micro_F1_val: 0.0331 time: 4.4979s\n",
      "Epoch: 0035 loss_train: 5.5399 micro_F1_train: 0.0566 loss_val: 0.6940 micro_F1_val: 0.0273 time: 3.5561s\n",
      "Epoch: 0036 loss_train: 5.5360 micro_F1_train: 0.0700 loss_val: 0.6918 micro_F1_val: 0.0904 time: 3.1616s\n",
      "Epoch: 0037 loss_train: 5.5326 micro_F1_train: 0.1007 loss_val: 0.6921 micro_F1_val: 0.0950 time: 3.3979s\n",
      "Epoch: 0038 loss_train: 5.5310 micro_F1_train: 0.1131 loss_val: 0.6919 micro_F1_val: 0.0904 time: 3.1388s\n",
      "Epoch: 0039 loss_train: 5.7891 micro_F1_train: 0.2515 loss_val: 0.7264 micro_F1_val: 0.2256 time: 3.0266s\n",
      "Epoch: 0040 loss_train: 5.7897 micro_F1_train: 0.2705 loss_val: 0.7271 micro_F1_val: 0.2541 time: 2.9537s\n",
      "Epoch: 0041 loss_train: 5.7876 micro_F1_train: 0.2911 loss_val: 0.7256 micro_F1_val: 0.2667 time: 2.7299s\n",
      "Epoch: 0042 loss_train: 5.7885 micro_F1_train: 0.2984 loss_val: 0.7263 micro_F1_val: 0.2756 time: 2.7667s\n",
      "Epoch: 0043 loss_train: 5.7890 micro_F1_train: 0.2971 loss_val: 0.7266 micro_F1_val: 0.2677 time: 2.6627s\n",
      "Epoch: 0044 loss_train: 5.7752 micro_F1_train: 0.3108 loss_val: 0.7288 micro_F1_val: 0.2713 time: 2.8015s\n",
      "Epoch: 0045 loss_train: 5.7850 micro_F1_train: 0.2988 loss_val: 0.7277 micro_F1_val: 0.2679 time: 2.6935s\n",
      "Epoch: 0046 loss_train: 5.7790 micro_F1_train: 0.2960 loss_val: 0.7263 micro_F1_val: 0.2582 time: 2.7197s\n",
      "Epoch: 0047 loss_train: 5.7783 micro_F1_train: 0.2946 loss_val: 0.7242 micro_F1_val: 0.2745 time: 2.7990s\n",
      "Epoch: 0048 loss_train: 5.7760 micro_F1_train: 0.2859 loss_val: 0.7258 micro_F1_val: 0.2562 time: 3.0121s\n",
      "Epoch: 0049 loss_train: 5.7722 micro_F1_train: 0.2887 loss_val: 0.7250 micro_F1_val: 0.2727 time: 2.9882s\n",
      "Epoch: 0050 loss_train: 5.7707 micro_F1_train: 0.2858 loss_val: 0.7232 micro_F1_val: 0.2791 time: 2.9646s\n",
      "Epoch: 0051 loss_train: 5.7688 micro_F1_train: 0.2885 loss_val: 0.7250 micro_F1_val: 0.2623 time: 2.9441s\n",
      "Epoch: 0052 loss_train: 5.8062 micro_F1_train: 0.2741 loss_val: 0.7303 micro_F1_val: 0.2280 time: 2.8356s\n",
      "Epoch: 0053 loss_train: 5.8067 micro_F1_train: 0.2656 loss_val: 0.7286 micro_F1_val: 0.2492 time: 2.9149s\n",
      "Epoch: 0054 loss_train: 5.8098 micro_F1_train: 0.2500 loss_val: 0.7298 micro_F1_val: 0.2152 time: 3.0677s\n",
      "Epoch: 0055 loss_train: 5.8057 micro_F1_train: 0.2420 loss_val: 0.7283 micro_F1_val: 0.2061 time: 3.0521s\n",
      "Epoch: 0056 loss_train: 5.5383 micro_F1_train: 0.0894 loss_val: 0.6929 micro_F1_val: 0.0942 time: 3.3873s\n",
      "Epoch: 0057 loss_train: 5.5438 micro_F1_train: 0.0673 loss_val: 0.6930 micro_F1_val: 0.0489 time: 3.5149s\n",
      "Epoch: 0058 loss_train: 5.5372 micro_F1_train: 0.0724 loss_val: 0.6932 micro_F1_val: 0.0541 time: 3.4044s\n",
      "Epoch: 0059 loss_train: 5.5441 micro_F1_train: 0.0464 loss_val: 0.6935 micro_F1_val: 0.0329 time: 3.2658s\n",
      "Epoch: 0060 loss_train: 5.5394 micro_F1_train: 0.0453 loss_val: 0.6924 micro_F1_val: 0.0387 time: 29.7961s\n",
      "Epoch: 0061 loss_train: 5.5392 micro_F1_train: 0.0413 loss_val: 0.6916 micro_F1_val: 0.0549 time: 17.9272s\n",
      "Epoch: 0062 loss_train: 5.5390 micro_F1_train: 0.0331 loss_val: 0.6922 micro_F1_val: 0.0333 time: 3.5376s\n",
      "Epoch: 0063 loss_train: 5.5415 micro_F1_train: 0.0275 loss_val: 0.6928 micro_F1_val: 0.0223 time: 3.5027s\n",
      "Epoch: 0064 loss_train: 5.5417 micro_F1_train: 0.0213 loss_val: 0.6927 micro_F1_val: 0.0331 time: 3.1944s\n",
      "Epoch: 0065 loss_train: 5.5401 micro_F1_train: 0.0276 loss_val: 0.6924 micro_F1_val: 0.0279 time: 3.4538s\n",
      "Epoch: 0066 loss_train: 5.5384 micro_F1_train: 0.0311 loss_val: 0.6928 micro_F1_val: 0.0112 time: 3.1876s\n",
      "Epoch: 0067 loss_train: 5.5373 micro_F1_train: 0.0373 loss_val: 0.6925 micro_F1_val: 0.0223 time: 3.3224s\n",
      "Epoch: 0068 loss_train: 5.5343 micro_F1_train: 0.0530 loss_val: 0.6922 micro_F1_val: 0.0388 time: 3.0231s\n",
      "Epoch: 0069 loss_train: 5.5312 micro_F1_train: 0.0685 loss_val: 0.6923 micro_F1_val: 0.0386 time: 3.4527s\n",
      "Epoch: 0070 loss_train: 5.5210 micro_F1_train: 0.1207 loss_val: 0.6913 micro_F1_val: 0.0909 time: 32.6780s\n",
      "Epoch: 0071 loss_train: 5.5085 micro_F1_train: 0.1781 loss_val: 0.6901 micro_F1_val: 0.1214 time: 2.8716s\n",
      "Epoch: 0072 loss_train: 5.7579 micro_F1_train: 0.3105 loss_val: 0.7218 micro_F1_val: 0.2834 time: 4.4269s\n",
      "Epoch: 0073 loss_train: 5.7594 micro_F1_train: 0.3142 loss_val: 0.7228 micro_F1_val: 0.2866 time: 3.4158s\n",
      "Epoch: 0074 loss_train: 5.7542 micro_F1_train: 0.3265 loss_val: 0.7233 micro_F1_val: 0.2908 time: 2.8829s\n",
      "Epoch: 0075 loss_train: 5.7532 micro_F1_train: 0.3273 loss_val: 0.7226 micro_F1_val: 0.2922 time: 2.6740s\n",
      "Epoch: 0076 loss_train: 5.7551 micro_F1_train: 0.3298 loss_val: 0.7221 micro_F1_val: 0.2959 time: 2.6498s\n",
      "Epoch: 0077 loss_train: 5.7517 micro_F1_train: 0.3319 loss_val: 0.7218 micro_F1_val: 0.3010 time: 2.6955s\n",
      "Epoch: 0078 loss_train: 5.7537 micro_F1_train: 0.3303 loss_val: 0.7223 micro_F1_val: 0.3027 time: 2.7883s\n",
      "Epoch: 0079 loss_train: 5.6474 micro_F1_train: 0.2943 loss_val: 0.7082 micro_F1_val: 0.2529 time: 2.7093s\n",
      "Epoch: 0080 loss_train: 5.5156 micro_F1_train: 0.1306 loss_val: 0.6892 micro_F1_val: 0.1414 time: 2.8690s\n",
      "Epoch: 0081 loss_train: 5.5434 micro_F1_train: 0.0079 loss_val: 0.6926 micro_F1_val: 0.0223 time: 3.2244s\n",
      "Epoch: 0082 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.8677s\n",
      "Epoch: 0083 loss_train: 5.5447 micro_F1_train: 0.0036 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6388s\n",
      "Epoch: 0084 loss_train: 5.5454 micro_F1_train: 0.0014 loss_val: 0.6929 micro_F1_val: 0.0112 time: 2.8177s\n",
      "Epoch: 0085 loss_train: 5.5460 micro_F1_train: 0.0036 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7981s\n",
      "Epoch: 0086 loss_train: 5.5458 micro_F1_train: 0.0064 loss_val: 0.6930 micro_F1_val: 0.0056 time: 2.8592s\n",
      "Epoch: 0087 loss_train: 5.5470 micro_F1_train: 0.0036 loss_val: 0.6932 micro_F1_val: 0.0000 time: 3.0283s\n",
      "Epoch: 0088 loss_train: 5.5468 micro_F1_train: 0.0064 loss_val: 0.6928 micro_F1_val: 0.0112 time: 3.0045s\n",
      "Epoch: 0089 loss_train: 5.5463 micro_F1_train: 0.0043 loss_val: 0.6930 micro_F1_val: 0.0056 time: 3.0629s\n",
      "Epoch: 0090 loss_train: 5.5466 micro_F1_train: 0.0064 loss_val: 0.6932 micro_F1_val: 0.0056 time: 2.8190s\n",
      "Epoch: 0091 loss_train: 5.5466 micro_F1_train: 0.0064 loss_val: 0.6938 micro_F1_val: 0.0111 time: 3.1303s\n",
      "Epoch: 0092 loss_train: 5.5466 micro_F1_train: 0.0043 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.8608s\n",
      "Epoch: 0093 loss_train: 5.5479 micro_F1_train: 0.0057 loss_val: 0.6932 micro_F1_val: 0.0056 time: 2.8853s\n",
      "Epoch: 0094 loss_train: 5.5455 micro_F1_train: 0.0086 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.8950s\n",
      "Epoch: 0095 loss_train: 5.5467 micro_F1_train: 0.0078 loss_val: 0.6939 micro_F1_val: 0.0000 time: 3.0085s\n",
      "Epoch: 0096 loss_train: 5.5469 micro_F1_train: 0.0078 loss_val: 0.6935 micro_F1_val: 0.0056 time: 3.2158s\n",
      "Epoch: 0097 loss_train: 5.5473 micro_F1_train: 0.0043 loss_val: 0.6937 micro_F1_val: 0.0000 time: 2.9587s\n",
      "Epoch: 0098 loss_train: 5.5471 micro_F1_train: 0.0064 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7177s\n",
      "Epoch: 0099 loss_train: 5.5470 micro_F1_train: 0.0057 loss_val: 0.6930 micro_F1_val: 0.0056 time: 2.9312s\n",
      "Epoch: 0100 loss_train: 5.5467 micro_F1_train: 0.0057 loss_val: 0.6936 micro_F1_val: 0.0000 time: 2.8259s\n",
      "Epoch: 0101 loss_train: 5.5452 micro_F1_train: 0.0021 loss_val: 0.6932 micro_F1_val: 0.0056 time: 2.9481s\n",
      "Epoch: 0102 loss_train: 5.5470 micro_F1_train: 0.0000 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.8358s\n",
      "Epoch: 0103 loss_train: 5.5462 micro_F1_train: 0.0036 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.6054s\n",
      "Epoch: 0104 loss_train: 5.5458 micro_F1_train: 0.0007 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7575s\n",
      "Epoch: 0105 loss_train: 5.5457 micro_F1_train: 0.0014 loss_val: 0.6930 micro_F1_val: 0.0056 time: 2.7619s\n",
      "Epoch: 0106 loss_train: 5.5459 micro_F1_train: 0.0000 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.7736s\n",
      "Epoch: 0107 loss_train: 5.5460 micro_F1_train: 0.0007 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.5692s\n",
      "Epoch: 0108 loss_train: 5.5456 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5923s\n",
      "Epoch: 0109 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6851s\n",
      "Epoch: 0110 loss_train: 5.5458 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5007s\n",
      "Epoch: 0111 loss_train: 5.5453 micro_F1_train: 0.0007 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6611s\n",
      "Epoch: 0112 loss_train: 5.5450 micro_F1_train: 0.0007 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6299s\n",
      "Epoch: 0113 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.2820s\n",
      "Epoch: 0114 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.9039s\n",
      "Epoch: 0115 loss_train: 5.5457 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6944s\n",
      "Epoch: 0116 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6934 micro_F1_val: 0.0000 time: 2.5718s\n",
      "Epoch: 0117 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4221s\n",
      "Epoch: 0118 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5960s\n",
      "Epoch: 0119 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5431s\n",
      "Epoch: 0120 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.9118s\n",
      "Epoch: 0121 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7880s\n",
      "Epoch: 0122 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5742s\n",
      "Epoch: 0123 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5471s\n",
      "Epoch: 0124 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4461s\n",
      "Epoch: 0125 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4464s\n",
      "Epoch: 0126 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6484s\n",
      "Epoch: 0127 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4881s\n",
      "Epoch: 0128 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5712s\n",
      "Epoch: 0129 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.3725s\n",
      "Epoch: 0130 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.2036s\n",
      "Epoch: 0131 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5678s\n",
      "Epoch: 0132 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5736s\n",
      "Epoch: 0133 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5115s\n",
      "Epoch: 0134 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5723s\n",
      "Epoch: 0135 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4653s\n",
      "Epoch: 0136 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7095s\n",
      "Epoch: 0137 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5812s\n",
      "Epoch: 0138 loss_train: 5.5457 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6172s\n",
      "Epoch: 0139 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5291s\n",
      "Epoch: 0140 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4867s\n",
      "Epoch: 0141 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.3998s\n",
      "Epoch: 0142 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5804s\n",
      "Epoch: 0143 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4990s\n",
      "Epoch: 0144 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4980s\n",
      "Epoch: 0145 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.3478s\n",
      "Epoch: 0146 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4601s\n",
      "Epoch: 0147 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5649s\n",
      "Epoch: 0148 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5488s\n",
      "Epoch: 0149 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5490s\n",
      "Epoch: 0150 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4862s\n",
      "Epoch: 0151 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.8063s\n",
      "Epoch: 0152 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4965s\n",
      "Epoch: 0153 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5850s\n",
      "Epoch: 0154 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6932 micro_F1_val: 0.0000 time: 2.4108s\n",
      "Epoch: 0155 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4473s\n",
      "Epoch: 0156 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4819s\n",
      "Epoch: 0157 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5646s\n",
      "Epoch: 0158 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4812s\n",
      "Epoch: 0159 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6057s\n",
      "Epoch: 0160 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4197s\n",
      "Epoch: 0161 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6228s\n",
      "Epoch: 0162 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5182s\n",
      "Epoch: 0163 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.1720s\n",
      "Epoch: 0164 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5521s\n",
      "Epoch: 0165 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6932 micro_F1_val: 0.0000 time: 2.4308s\n",
      "Epoch: 0166 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.9257s\n",
      "Epoch: 0167 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4850s\n",
      "Epoch: 0168 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.4396s\n",
      "Epoch: 0169 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.9349s\n",
      "Epoch: 0170 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.3798s\n",
      "Epoch: 0171 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7581s\n",
      "Epoch: 0172 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.3817s\n",
      "Epoch: 0173 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.3502s\n",
      "Epoch: 0174 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.6999s\n",
      "Epoch: 0175 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.2613s\n",
      "Epoch: 0176 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.4215s\n",
      "Epoch: 0177 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.0031s\n",
      "Epoch: 0178 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.4780s\n",
      "Epoch: 0179 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.6280s\n",
      "Epoch: 0180 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.6397s\n",
      "Epoch: 0181 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.9883s\n",
      "Epoch: 0182 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6785s\n",
      "Epoch: 0183 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5682s\n",
      "Epoch: 0184 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.8703s\n",
      "Epoch: 0185 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7698s\n",
      "Epoch: 0186 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7215s\n",
      "Epoch: 0187 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6752s\n",
      "Epoch: 0188 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6932 micro_F1_val: 0.0000 time: 2.6580s\n",
      "Epoch: 0189 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.7835s\n",
      "Epoch: 0190 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.8254s\n",
      "Epoch: 0191 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6099s\n",
      "Epoch: 0192 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5776s\n",
      "Epoch: 0193 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.0599s\n",
      "Epoch: 0194 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.9062s\n",
      "Epoch: 0195 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6581s\n",
      "Epoch: 0196 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5912s\n",
      "Epoch: 0197 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6560s\n",
      "Epoch: 0198 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6273s\n",
      "Epoch: 0199 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.8099s\n",
      "Epoch: 0200 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.9569s\n",
      "Epoch: 0201 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6556s\n",
      "Epoch: 0202 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5669s\n",
      "Epoch: 0203 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6054s\n",
      "Epoch: 0204 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6201s\n",
      "Epoch: 0205 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.6476s\n",
      "Epoch: 0206 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 2.5833s\n",
      "Epoch: 0207 loss_train: 5.5452 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.7203s\n",
      "Epoch: 0208 loss_train: 5.5454 micro_F1_train: 0.0000 loss_val: 0.6931 micro_F1_val: 0.0000 time: 3.0328s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-300e70501c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_together\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-42c3b8123386>\u001b[0m in \u001b[0;36mtrain_together\u001b[0;34m(epoch, M, N)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss_train_ll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss_train_hl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(300):\n",
    "    train_together(epoch, 50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'a.u.')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train_list,label='train')\n",
    "plt.plot(loss_val_list,label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('a.u.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'value')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(micro_F1_train_list,label='train')\n",
    "plt.plot(micro_F1_val_list,label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('micro_F1')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    low_layer.eval()\n",
    "    output_ll, X_embedding_ = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "    loss_test = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_test][:,i], labels[idx_test][:,i]) for i in range(m)])\n",
    "    micro_F1_train = micro_F1(threshold(output_ll.detach().numpy()[idx_test]), labels.detach().numpy()[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"micro_F1= {:.4f}\".format(micro_F1_train.item()))\n",
    "    return threshold(output_ll.detach().numpy()[idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 5.5452 micro_F1= 0.0000\n"
     ]
    }
   ],
   "source": [
    "y_predict = test()\n",
    "y_predict = y_predict.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.00      0.00      0.00        96\n",
      "           D       0.00      0.00      0.00       116\n",
      "           G       0.00      0.00      0.00        14\n",
      "           C       0.00      0.00      0.00        28\n",
      "           A       0.00      0.00      0.00        14\n",
      "           H       0.00      0.00      0.00         8\n",
      "           M       0.00      0.00      0.00        13\n",
      "           O       0.00      0.00      0.00        69\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       358\n",
      "   macro avg       0.00      0.00      0.00       358\n",
      "weighted avg       0.00      0.00      0.00       358\n",
      " samples avg       0.00      0.00      0.00       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = labels.numpy()[idx_test]\n",
    "target_names = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
    "print(classification_report(y_true, y_predict, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98742e5425ae56e82c6f35143b466ee53f026fe409428ca6213858b6727b8933"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
