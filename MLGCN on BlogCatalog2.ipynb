{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement MLGCN on BlogCatalog dataset\n",
    "\n",
    "* **Link to the paper:** https://arxiv.org/pdf/1912.11757.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import torch\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom made functions: \n",
    "`normalize` : used in the normalization step for the adjacency matrix\n",
    "\n",
    "`threshold` : used for defining the sigmoid output to 0 or 1\n",
    "\n",
    "`accuracy_sample_class` : accuracy for samples having multilabels\n",
    "\n",
    "`micor_F1` and `macro_F1` : built based on the equations provided in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(output):\n",
    "    output[output > 0.5] = 1\n",
    "    output[output <= 0.5] = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sample_class(output, labels):\n",
    "    \"\"\" \n",
    "    output is of shape (N,C)\n",
    "    Labels is of shape (N,C)\n",
    "    Result : acc gives the accuracy computed according to the sample-class view\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    C = labels.shape[1]\n",
    "    corr = np.sum(np.equal(output, labels))\n",
    "    # corr is the number of equal elements between labels and output and thus the number of correctly classified \n",
    "    # labels for each sample \n",
    "    acc = corr/(N*C)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_F1(output, labels):\n",
    "    TP = np.array([])\n",
    "    FN = np.array([])\n",
    "    FP = np.array([])\n",
    "    N = labels.shape[0]\n",
    "    L = labels.shape[1]\n",
    "    for l in range(L):\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        ol = output[:,l]\n",
    "        ll = labels[:,l]\n",
    "        for i in range(N):\n",
    "            if ll[i] == 1 and ol[i] == 1:\n",
    "                tp +=1\n",
    "            if ol[i] == 0 and ll[i] == 1:\n",
    "                fn +=1\n",
    "            if ol[i] == 1 and ll[i] == 0:\n",
    "                fp +=1\n",
    "        TP = np.append(TP, tp)\n",
    "        FN = np.append(FN, fn)\n",
    "        FP = np.append(FP, fp)\n",
    "    return np.sum(2*TP)/np.sum(2*TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_F1(output, labels):\n",
    "    TP = np.array([])\n",
    "    FN = np.array([])\n",
    "    FP = np.array([])\n",
    "    N = labels.shape[0]\n",
    "    L = labels.shape[1]\n",
    "    for l in range(L):\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        fp = 0\n",
    "        ol = output[:,l]\n",
    "        ll = labels[:,l]\n",
    "        for i in range(N):\n",
    "            if ll[i] == 1 and ol[i] == 1:\n",
    "                tp +=1\n",
    "            if ol[i] == 0 and ll[i] == 1:\n",
    "                fn +=1\n",
    "            if ol[i] == 1 and ll[i] == 0:\n",
    "                fp +=1\n",
    "        TP = np.append(TP, tp)\n",
    "        FN = np.append(FN, fn)\n",
    "        FP = np.append(FP, fp)\n",
    "    return np.sum(2*TP/(2*TP + FN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Function:\n",
    "Creating the input feature matrices, adjacency matrices and embeddings as input to the model from the two given csv files in `BlogCatalog` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "m = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_name):\n",
    "    edges_file = data_name + \"/edges.csv\"\n",
    "    node_label_file = data_name + \"/group-edges.csv\"\n",
    "    label_occ_file = data_name + \"/label_co-occurences.csv\"\n",
    "    nnlg_file = data_name + \"/edges_node_node_label.csv\"\n",
    "    llng_file = data_name + \"/edges_label_label_node.csv\"\n",
    "    label_raw, nodes = [], []\n",
    "    with open(node_label_file) as file_to_read: \n",
    "        while True:\n",
    "            lines = file_to_read.readline()\n",
    "            if not lines:\n",
    "                break \n",
    "            node, label = lines.split(\",\")\n",
    "            label_raw.append(int(label))\n",
    "            nodes.append(int(node))\n",
    "    label_raw = np.array(label_raw)\n",
    "    nodes = np.array(nodes)\n",
    "    unique_nodes = np.unique(nodes)\n",
    "    labels = np.zeros((unique_nodes.shape[0], m))\n",
    "    for l in range(1, m+1, 1):\n",
    "        indices = np.argwhere(label_raw == l).reshape(-1)\n",
    "        n_l = nodes[indices]\n",
    "        for n in n_l:\n",
    "            labels[n-1][l-1] = 1\n",
    "    \n",
    "    # node-node-label graph \n",
    "    label_nodes = label_raw + unique_nodes.shape[0] \n",
    "    n_n_l_nodes = np.concatenate((unique_nodes, np.unique(label_nodes)))\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(nnlg_file)\n",
    "    f = open(nnlg_file, \"r+\")\n",
    "    file_to_read = open(edges_file, \"r\")\n",
    "    f.writelines(file_to_read.readlines())\n",
    "    a = np.dstack((nodes, label_nodes)).reshape(label_nodes.shape[0],2)\n",
    "    e = [\"\\n\"] + [\",\".join(item)+\"\\n\" for item in a.astype(str)]\n",
    "    f.writelines(e)\n",
    "    f.close()\n",
    "    \n",
    "    nnlg_file = \"BlogCatalog/edges_node_node_label.csv\"\n",
    "    nnl_graph = nx.read_edgelist(nnlg_file, delimiter = \",\", nodetype = int)\n",
    "    E = nx.adjacency_matrix(nnl_graph, nodelist = n_n_l_nodes)\n",
    "    main_graph = open(edges_file, \"rb\")\n",
    "    G = nx.read_edgelist(main_graph, delimiter = \",\", nodetype = int)\n",
    "    A = nx.adjacency_matrix(G, nodelist = unique_nodes)\n",
    "    A = sp.coo_matrix(A.todense())\n",
    "    # the input node feature is identity matrix now\n",
    "    X = np.eye(A.shape[0])\n",
    "    X_embedding = X[:,:400]\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    A_tilde = normalize(A + 3*sp.eye(A.shape[0]))\n",
    "    \n",
    "    # Label-label-node graph \n",
    "    edges = []\n",
    "    list_edges = []\n",
    "    for k in range(labels.shape[0]):\n",
    "        indices = np.argwhere(labels[k] == 1).reshape(-1)\n",
    "        if indices.shape[0]>1:\n",
    "            for subset in itertools.combinations(indices, 2): \n",
    "                if (list(subset) not in list_edges) or ([subset[1], subset[0]] not in list_edges):\n",
    "                    list_edges.append([subset[0], subset[1]+labels.shape[0]])# check if the common nodes should be before or after the label nodes\n",
    "                    edges.append(str(subset[0]+1) + \",\" + str(subset[1] +1) + \"\\n\")\n",
    "    df = pd.DataFrame(list())\n",
    "    df.to_csv(label_occ_file)\n",
    "    label_file = open(label_occ_file, \"r+\")\n",
    "    label_file.writelines(edges)\n",
    "    label_file.close()\n",
    "    unique_label_ID = np.arange(1,40)\n",
    "    label_file = open(label_occ_file, \"rb\")\n",
    "    label_graph = nx.read_edgelist(label_file, delimiter = \",\", nodetype = int)\n",
    "    C = nx.adjacency_matrix(label_graph, nodelist = unique_label_ID)\n",
    "    label_file.close()\n",
    "    # Normalize the co-occurence matrix with the added self loops\n",
    "    C_tilde = normalize(C + 2*sp.eye(C.shape[0]))\n",
    "    Y = np.random.rand(m,X.shape[0]) #instead of X[:m]\n",
    "    Y_embedding = Y[:,:400]\n",
    "    labels_ind = label_raw \n",
    "    nodes_ind = nodes + labels.shape[1]\n",
    "    a_1 = np.dstack((labels_ind,nodes)).reshape(labels_ind.shape[0],2)\n",
    "    e_1 = [\",\".join(item)+\"\\n\" for item in a_1.astype(str)]\n",
    "    file = open(label_occ_file, \"r+\")\n",
    "    file.writelines(e_1)\n",
    "    file.close()\n",
    "    f_1 = open(label_occ_file, \"rb\")\n",
    "    l_l_n_nodes = np.concatenate((np.unique(nodes_ind),np.unique(labels_ind)))\n",
    "    lln_graph = nx.read_edgelist(f_1, delimiter = \",\", nodetype = int)\n",
    "    F = nx.adjacency_matrix(lln_graph, nodelist = l_l_n_nodes)\n",
    "    F = sp.coo_matrix(F.todense())\n",
    "    f_1.close()\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    E = normalize(E + 3*sp.eye(E.shape[0]))\n",
    "    E_tilde = E[:len(unique_nodes)]\n",
    "    # Normalize the adjancency matrix with the added self loops\n",
    "    F = normalize(F + 2*sp.eye(F.shape[0]))\n",
    "    F_tilde = F[len(unique_nodes):]\n",
    "    \n",
    "    indices = np.arange(A.shape[0]).astype('int32') # should be shuffled\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    idx_train = indices[:(4 * A.shape[0]) // 5]\n",
    "    idx_val = indices[(4 * A.shape[0]) // 5 : (9 * A.shape[0]) // 10]\n",
    "    idx_test = indices[(9 * A.shape[0]) // 10:]\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    C_tilde = torch.FloatTensor(np.array(C_tilde.todense()))\n",
    "    E_tilde = torch.FloatTensor(np.array(E_tilde.todense()))\n",
    "    F_tilde = torch.FloatTensor(np.array(F_tilde.todense()))\n",
    "    A_tilde = torch.FloatTensor(np.array(A_tilde.todense()))\n",
    "    labels = torch.FloatTensor(labels)\n",
    "    Y = torch.FloatTensor(Y)\n",
    "    X = torch.FloatTensor(X)\n",
    "    X_embedding = torch.FloatTensor(X_embedding)\n",
    "    Y_embedding = torch.FloatTensor(Y_embedding)\n",
    "    \n",
    "    return G, X, Y, X_embedding, Y_embedding, F_tilde, E_tilde, C_tilde,A_tilde, idx_train, idx_val,idx_test, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, X, Y, X_embedding, Y_embedding, F_tilde, E_tilde, C_tilde,A_tilde, idx_train, idx_val,idx_test, labels = load_data(\"BlogCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables \n",
    "n = A_tilde.shape[0]\n",
    "m = C_tilde.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get down to the training \n",
    "\n",
    "The cell below contains the models importing from `models1.py`, if the **Jupyter Notebook IDE** is used,\n",
    "\n",
    "change `from models1 import High_Layer, Low_Layer, GCN` \n",
    "\n",
    "to `from MLMI_MLGCN_project.models1 import High_Layer, Low_Layer, GCN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from models1 import High_Layer, Low_Layer, GCN\n",
    "\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=300,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=400,\n",
    "                    help='Number of hidden units.')                   \n",
    "parser.add_argument('--hidden_ll', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--hidden_hl', type=int, default=400,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--hidden1', type=int, default=1000,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models (high layer and low layer) and the optimizers for each on of them\n",
    "low_layer = Low_Layer(nfeat = A_tilde.shape[0],\n",
    "                      nhid_lowlayer = args.hidden_ll,\n",
    "                      nhid_highlayer = args.hidden_hl,\n",
    "                      nclass = C_tilde.shape[0],\n",
    "                      dropout = args.dropout)\n",
    "optimizer_lowlayer = optim.Adam(low_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)\n",
    "        \n",
    "high_layer = High_Layer(nfeat = A_tilde.shape[0],\n",
    "                        nhid_lowlayer = args.hidden_ll,\n",
    "                        nhid_highlayer = args.hidden_hl,\n",
    "                        nclass = C_tilde.shape[0],\n",
    "                        dropout = args.dropout)\n",
    "optimizer_highlayer = optim.Adam(high_layer.parameters(),lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = torch.LongTensor(np.arange(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Separate Training Parts\n",
    "\n",
    "Define the trainig part for low_layer as train_low and high_layer as train_high, this will make the iterating between two layers easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_list = []\n",
    "micro_F1_train_list = []\n",
    "loss_val_list = []\n",
    "micro_F1_val_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_low(epoch):\n",
    "      global loss_train_list \n",
    "      global micro_F1_train_list \n",
    "      global loss_val_list\n",
    "      global micro_F1_val_list\n",
    "      low_layer.train()\n",
    "      optimizer_lowlayer.zero_grad()\n",
    "      output_ll, X_embedding_ = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "    \n",
    "      X_embedding_ = X_embedding_.detach()\n",
    "    \n",
    "      loss_train_ll = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_train][:,i], labels[idx_train][:,i]) for i in range(m)])\n",
    "      micro_F1_train = micro_F1(threshold(output_ll.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "      loss_train_ll.backward()\n",
    "      optimizer_lowlayer.step()\n",
    "    \n",
    "      # validation part\n",
    "      low_layer.eval()\n",
    "      output_ll, X_embedding_val = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "      loss_val = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_val][:,i], labels[idx_val][:,i]) for i in range(m)])\n",
    "      micro_F1_val = micro_F1(threshold(output_ll.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "\n",
    "      loss_train_list.append(loss_train_ll.item())\n",
    "      loss_val_list.append(loss_val.item())\n",
    "      micro_F1_train_list.append(micro_F1_train.item())\n",
    "      micro_F1_val_list.append(micro_F1_val.item())\n",
    "\n",
    "      print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_ll.item()),\n",
    "          'micro_F1_train: {:.4f}'.format(micro_F1_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'micro_F1_val: {:.4f}'.format(micro_F1_val.item())\n",
    "          )\n",
    "\n",
    "      return X_embedding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_high(epoch):\n",
    "    high_layer.train()\n",
    "    optimizer_highlayer.zero_grad()\n",
    "    output_hl, Y_embedding_ = high_layer(X_, Y, F_tilde, C_tilde)\n",
    "    Y_embedding_ = Y_embedding_.detach()\n",
    "    loss_train_hl = F.nll_loss(output_hl, truth)\n",
    "    loss_train_hl.backward()\n",
    "    optimizer_highlayer.step()\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train_hl.item()),\n",
    "          #'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          )\n",
    "    \n",
    "    return Y_embedding_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "This part is for training the two layers GCN, which contains two parts:\n",
    "\n",
    "`train_embedding` : we first train the meaningful embedding to give it as the input to high_layer, # training epochs = 50.\n",
    "\n",
    "`train_sep` : training high_layer and low_layer one by one, start with high_layer, each layer will be train for 50 epochs as suggested in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_GCN = GCN(nfeat = X.shape[1],\n",
    "                      nhid = args.hidden,\n",
    "                      nclass = m,\n",
    "                      dropout = args.dropout)\n",
    "optimizer_emb = optim.Adam(simple_GCN.parameters(),lr = args.lr, weight_decay = args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(epoch):\n",
    "    simple_GCN.train()\n",
    "    optimizer_emb.zero_grad()\n",
    "    output, X_embedding = simple_GCN(X, A_tilde)\n",
    "    loss_train = np.sum([F.binary_cross_entropy_with_logits(output[idx_train][:,i], labels[idx_train][:,i]) for i in range(39)])\n",
    "    micro_F1_train = micro_F1(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    # macro_F1_train = macro_F1(threshold(output.detach().numpy()[idx_train]), labels.detach().numpy()[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer_emb.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        simple_GCN.eval()\n",
    "        output, X_embedding_val = simple_GCN(X, A_tilde)\n",
    "\n",
    "    loss_val = np.sum([F.binary_cross_entropy_with_logits(output[idx_val][:,i], labels[idx_val][:,i]) for i in range(39)])\n",
    "    micro_F1_val = micro_F1(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    # macro_F1_val = macro_F1(threshold(output.detach().numpy()[idx_val]), labels.detach().numpy()[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'micro_F1_train: {:.4f}'.format(micro_F1_train.item()),\n",
    "        #   'macro_F1_train: {:.4f}'.format(macro_F1_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'micro_F1_val: {:.4f}'.format(micro_F1_val.item())\n",
    "        #   'macro_F1_val: {:.4f}'.format(macro_F1_val.item())\n",
    "          )\n",
    "    return X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 37.3034 micro_F1_train: 0.0757 loss_val: 37.3005 micro_F1_val: 0.0743\n",
      "Epoch: 0002 loss_train: 37.2986 micro_F1_train: 0.0763 loss_val: 37.2948 micro_F1_val: 0.0740\n",
      "Epoch: 0003 loss_train: 37.2928 micro_F1_train: 0.0754 loss_val: 37.2892 micro_F1_val: 0.0740\n",
      "Epoch: 0004 loss_train: 37.2877 micro_F1_train: 0.0749 loss_val: 37.2835 micro_F1_val: 0.0741\n",
      "Epoch: 0005 loss_train: 37.2795 micro_F1_train: 0.0755 loss_val: 37.2778 micro_F1_val: 0.0742\n",
      "Epoch: 0006 loss_train: 37.2776 micro_F1_train: 0.0755 loss_val: 37.2721 micro_F1_val: 0.0742\n",
      "Epoch: 0007 loss_train: 37.2691 micro_F1_train: 0.0745 loss_val: 37.2665 micro_F1_val: 0.0742\n",
      "Epoch: 0008 loss_train: 37.2643 micro_F1_train: 0.0751 loss_val: 37.2608 micro_F1_val: 0.0743\n",
      "Epoch: 0009 loss_train: 37.2568 micro_F1_train: 0.0745 loss_val: 37.2551 micro_F1_val: 0.0743\n",
      "Epoch: 0010 loss_train: 37.2528 micro_F1_train: 0.0750 loss_val: 37.2494 micro_F1_val: 0.0742\n",
      "Epoch: 0011 loss_train: 37.2482 micro_F1_train: 0.0752 loss_val: 37.2437 micro_F1_val: 0.0742\n",
      "Epoch: 0012 loss_train: 37.2426 micro_F1_train: 0.0745 loss_val: 37.2380 micro_F1_val: 0.0742\n",
      "Epoch: 0013 loss_train: 37.2351 micro_F1_train: 0.0746 loss_val: 37.2322 micro_F1_val: 0.0742\n",
      "Epoch: 0014 loss_train: 37.2291 micro_F1_train: 0.0745 loss_val: 37.2265 micro_F1_val: 0.0742\n",
      "Epoch: 0015 loss_train: 37.2240 micro_F1_train: 0.0745 loss_val: 37.2207 micro_F1_val: 0.0741\n",
      "Epoch: 0016 loss_train: 37.2179 micro_F1_train: 0.0743 loss_val: 37.2149 micro_F1_val: 0.0742\n",
      "Epoch: 0017 loss_train: 37.2127 micro_F1_train: 0.0747 loss_val: 37.2092 micro_F1_val: 0.0743\n",
      "Epoch: 0018 loss_train: 37.2074 micro_F1_train: 0.0751 loss_val: 37.2034 micro_F1_val: 0.0744\n",
      "Epoch: 0019 loss_train: 37.2005 micro_F1_train: 0.0746 loss_val: 37.1975 micro_F1_val: 0.0744\n",
      "Epoch: 0020 loss_train: 37.1920 micro_F1_train: 0.0749 loss_val: 37.1917 micro_F1_val: 0.0746\n",
      "Epoch: 0021 loss_train: 37.1879 micro_F1_train: 0.0758 loss_val: 37.1858 micro_F1_val: 0.0742\n",
      "Epoch: 0022 loss_train: 37.1845 micro_F1_train: 0.0748 loss_val: 37.1799 micro_F1_val: 0.0736\n",
      "Epoch: 0023 loss_train: 37.1762 micro_F1_train: 0.0750 loss_val: 37.1740 micro_F1_val: 0.0738\n",
      "Epoch: 0024 loss_train: 37.1746 micro_F1_train: 0.0754 loss_val: 37.1680 micro_F1_val: 0.0740\n",
      "Epoch: 0025 loss_train: 37.1677 micro_F1_train: 0.0752 loss_val: 37.1621 micro_F1_val: 0.0740\n",
      "Epoch: 0026 loss_train: 37.1580 micro_F1_train: 0.0749 loss_val: 37.1561 micro_F1_val: 0.0742\n",
      "Epoch: 0027 loss_train: 37.1530 micro_F1_train: 0.0765 loss_val: 37.1501 micro_F1_val: 0.0747\n",
      "Epoch: 0028 loss_train: 37.1472 micro_F1_train: 0.0758 loss_val: 37.1440 micro_F1_val: 0.0757\n",
      "Epoch: 0029 loss_train: 37.1432 micro_F1_train: 0.0762 loss_val: 37.1379 micro_F1_val: 0.0761\n",
      "Epoch: 0030 loss_train: 37.1354 micro_F1_train: 0.0771 loss_val: 37.1318 micro_F1_val: 0.0765\n",
      "Epoch: 0031 loss_train: 37.1306 micro_F1_train: 0.0769 loss_val: 37.1257 micro_F1_val: 0.0773\n",
      "Epoch: 0032 loss_train: 37.1230 micro_F1_train: 0.0782 loss_val: 37.1195 micro_F1_val: 0.0785\n",
      "Epoch: 0033 loss_train: 37.1181 micro_F1_train: 0.0779 loss_val: 37.1133 micro_F1_val: 0.0795\n",
      "Epoch: 0034 loss_train: 37.1116 micro_F1_train: 0.0794 loss_val: 37.1071 micro_F1_val: 0.0800\n",
      "Epoch: 0035 loss_train: 37.1059 micro_F1_train: 0.0796 loss_val: 37.1008 micro_F1_val: 0.0805\n",
      "Epoch: 0036 loss_train: 37.0974 micro_F1_train: 0.0802 loss_val: 37.0945 micro_F1_val: 0.0812\n",
      "Epoch: 0037 loss_train: 37.0901 micro_F1_train: 0.0794 loss_val: 37.0882 micro_F1_val: 0.0820\n",
      "Epoch: 0038 loss_train: 37.0876 micro_F1_train: 0.0818 loss_val: 37.0818 micro_F1_val: 0.0838\n",
      "Epoch: 0039 loss_train: 37.0777 micro_F1_train: 0.0812 loss_val: 37.0753 micro_F1_val: 0.0856\n",
      "Epoch: 0040 loss_train: 37.0739 micro_F1_train: 0.0829 loss_val: 37.0689 micro_F1_val: 0.0871\n",
      "Epoch: 0041 loss_train: 37.0663 micro_F1_train: 0.0812 loss_val: 37.0624 micro_F1_val: 0.0877\n",
      "Epoch: 0042 loss_train: 37.0604 micro_F1_train: 0.0835 loss_val: 37.0558 micro_F1_val: 0.0873\n",
      "Epoch: 0043 loss_train: 37.0559 micro_F1_train: 0.0847 loss_val: 37.0492 micro_F1_val: 0.0882\n",
      "Epoch: 0044 loss_train: 37.0491 micro_F1_train: 0.0842 loss_val: 37.0425 micro_F1_val: 0.0884\n",
      "Epoch: 0045 loss_train: 37.0406 micro_F1_train: 0.0818 loss_val: 37.0358 micro_F1_val: 0.0886\n",
      "Epoch: 0046 loss_train: 37.0323 micro_F1_train: 0.0840 loss_val: 37.0290 micro_F1_val: 0.0882\n",
      "Epoch: 0047 loss_train: 37.0241 micro_F1_train: 0.0827 loss_val: 37.0222 micro_F1_val: 0.0873\n",
      "Epoch: 0048 loss_train: 37.0170 micro_F1_train: 0.0803 loss_val: 37.0153 micro_F1_val: 0.0862\n",
      "Epoch: 0049 loss_train: 37.0124 micro_F1_train: 0.0823 loss_val: 37.0084 micro_F1_val: 0.0822\n",
      "Epoch: 0050 loss_train: 37.0043 micro_F1_train: 0.0755 loss_val: 37.0014 micro_F1_val: 0.0752\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    X_embedding = train_embedding(epoch)\n",
    "\n",
    "X_embedding = X_embedding.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X_embedding.clone()\n",
    "Y_ = Y_embedding.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0383, 0.0000, 0.0000,  ..., 0.0151, 0.0391, 0.0190],\n",
       "        [0.0367, 0.0000, 0.0000,  ..., 0.0184, 0.0450, 0.0022],\n",
       "        [0.0411, 0.0000, 0.0000,  ..., 0.0182, 0.0443, 0.0198],\n",
       "        ...,\n",
       "        [0.0346, 0.0000, 0.0000,  ..., 0.0195, 0.0323, 0.0202],\n",
       "        [0.0218, 0.0000, 0.0000,  ..., 0.0147, 0.0351, 0.0072],\n",
       "        [0.0144, 0.0000, 0.0000,  ..., 0.0103, 0.0436, 0.0131]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train them separately\n",
    "def train_sep(epoch, M, N):\n",
    "    global Y_\n",
    "    global X_\n",
    "\n",
    "    if np.floor(epoch/M) % 2 == 0:\n",
    "        Y_ = train_high(epoch)\n",
    "        \n",
    "    if np.floor(epoch/M) % 2 == 1:\n",
    "        X_ = train_low(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 3.6762\n",
      "Epoch: 0002 loss_train: 3.6641\n",
      "Epoch: 0003 loss_train: 3.6641\n",
      "Epoch: 0004 loss_train: 3.6655\n",
      "Epoch: 0005 loss_train: 3.6738\n",
      "Epoch: 0006 loss_train: 3.6663\n",
      "Epoch: 0007 loss_train: 3.6650\n",
      "Epoch: 0008 loss_train: 3.6662\n",
      "Epoch: 0009 loss_train: 3.6583\n",
      "Epoch: 0010 loss_train: 3.6598\n",
      "Epoch: 0011 loss_train: 3.6616\n",
      "Epoch: 0012 loss_train: 3.6619\n",
      "Epoch: 0013 loss_train: 3.6628\n",
      "Epoch: 0014 loss_train: 3.6617\n",
      "Epoch: 0015 loss_train: 3.6639\n",
      "Epoch: 0016 loss_train: 3.6619\n",
      "Epoch: 0017 loss_train: 3.6659\n",
      "Epoch: 0018 loss_train: 3.6697\n",
      "Epoch: 0019 loss_train: 3.6663\n",
      "Epoch: 0020 loss_train: 3.6597\n",
      "Epoch: 0021 loss_train: 3.6556\n",
      "Epoch: 0022 loss_train: 3.6603\n",
      "Epoch: 0023 loss_train: 3.6615\n",
      "Epoch: 0024 loss_train: 3.6623\n",
      "Epoch: 0025 loss_train: 3.6591\n",
      "Epoch: 0026 loss_train: 3.6619\n",
      "Epoch: 0027 loss_train: 3.6562\n",
      "Epoch: 0028 loss_train: 3.6661\n",
      "Epoch: 0029 loss_train: 3.6585\n",
      "Epoch: 0030 loss_train: 3.6646\n",
      "Epoch: 0031 loss_train: 3.6611\n",
      "Epoch: 0032 loss_train: 3.6678\n",
      "Epoch: 0033 loss_train: 3.6551\n",
      "Epoch: 0034 loss_train: 3.6620\n",
      "Epoch: 0035 loss_train: 3.6593\n",
      "Epoch: 0036 loss_train: 3.6621\n",
      "Epoch: 0037 loss_train: 3.6629\n",
      "Epoch: 0038 loss_train: 3.6572\n",
      "Epoch: 0039 loss_train: 3.6556\n",
      "Epoch: 0040 loss_train: 3.6569\n",
      "Epoch: 0041 loss_train: 3.6666\n",
      "Epoch: 0042 loss_train: 3.6656\n",
      "Epoch: 0043 loss_train: 3.6592\n",
      "Epoch: 0044 loss_train: 3.6587\n",
      "Epoch: 0045 loss_train: 3.6638\n",
      "Epoch: 0046 loss_train: 3.6583\n",
      "Epoch: 0047 loss_train: 3.6598\n",
      "Epoch: 0048 loss_train: 3.6570\n",
      "Epoch: 0049 loss_train: 3.6684\n",
      "Epoch: 0050 loss_train: 3.6636\n",
      "Epoch: 0051 loss_train: 37.2520 micro_F1_train: 0.0533 loss_val: 37.1993 micro_F1_val: 0.0497\n",
      "Epoch: 0052 loss_train: 37.1965 micro_F1_train: 0.0530 loss_val: 37.1469 micro_F1_val: 0.0494\n",
      "Epoch: 0053 loss_train: 37.1415 micro_F1_train: 0.0531 loss_val: 37.0953 micro_F1_val: 0.0498\n",
      "Epoch: 0054 loss_train: 37.0905 micro_F1_train: 0.0535 loss_val: 37.0439 micro_F1_val: 0.0498\n",
      "Epoch: 0055 loss_train: 37.0432 micro_F1_train: 0.0533 loss_val: 36.9920 micro_F1_val: 0.0515\n",
      "Epoch: 0056 loss_train: 36.9908 micro_F1_train: 0.0539 loss_val: 36.9389 micro_F1_val: 0.0529\n",
      "Epoch: 0057 loss_train: 36.9390 micro_F1_train: 0.0546 loss_val: 36.8845 micro_F1_val: 0.0550\n",
      "Epoch: 0058 loss_train: 36.8837 micro_F1_train: 0.0551 loss_val: 36.8284 micro_F1_val: 0.0558\n",
      "Epoch: 0059 loss_train: 36.8297 micro_F1_train: 0.0553 loss_val: 36.7707 micro_F1_val: 0.0560\n",
      "Epoch: 0060 loss_train: 36.7733 micro_F1_train: 0.0542 loss_val: 36.7114 micro_F1_val: 0.0576\n",
      "Epoch: 0061 loss_train: 36.7132 micro_F1_train: 0.0555 loss_val: 36.6504 micro_F1_val: 0.0583\n",
      "Epoch: 0062 loss_train: 36.6549 micro_F1_train: 0.0556 loss_val: 36.5878 micro_F1_val: 0.0596\n",
      "Epoch: 0063 loss_train: 36.5955 micro_F1_train: 0.0569 loss_val: 36.5235 micro_F1_val: 0.0606\n",
      "Epoch: 0064 loss_train: 36.5290 micro_F1_train: 0.0559 loss_val: 36.4574 micro_F1_val: 0.0620\n",
      "Epoch: 0065 loss_train: 36.4684 micro_F1_train: 0.0588 loss_val: 36.3897 micro_F1_val: 0.0613\n",
      "Epoch: 0066 loss_train: 36.3985 micro_F1_train: 0.0580 loss_val: 36.3202 micro_F1_val: 0.0620\n",
      "Epoch: 0067 loss_train: 36.3364 micro_F1_train: 0.0575 loss_val: 36.2489 micro_F1_val: 0.0614\n",
      "Epoch: 0068 loss_train: 36.2650 micro_F1_train: 0.0602 loss_val: 36.1758 micro_F1_val: 0.0622\n",
      "Epoch: 0069 loss_train: 36.1951 micro_F1_train: 0.0589 loss_val: 36.1009 micro_F1_val: 0.0630\n",
      "Epoch: 0070 loss_train: 36.1222 micro_F1_train: 0.0573 loss_val: 36.0242 micro_F1_val: 0.0629\n",
      "Epoch: 0071 loss_train: 36.0463 micro_F1_train: 0.0551 loss_val: 35.9456 micro_F1_val: 0.0629\n",
      "Epoch: 0072 loss_train: 35.9715 micro_F1_train: 0.0568 loss_val: 35.8651 micro_F1_val: 0.0616\n",
      "Epoch: 0073 loss_train: 35.8966 micro_F1_train: 0.0569 loss_val: 35.7828 micro_F1_val: 0.0626\n",
      "Epoch: 0074 loss_train: 35.8153 micro_F1_train: 0.0553 loss_val: 35.6986 micro_F1_val: 0.0641\n",
      "Epoch: 0075 loss_train: 35.7319 micro_F1_train: 0.0567 loss_val: 35.6124 micro_F1_val: 0.0637\n",
      "Epoch: 0076 loss_train: 35.6499 micro_F1_train: 0.0559 loss_val: 35.5245 micro_F1_val: 0.0640\n",
      "Epoch: 0077 loss_train: 35.5711 micro_F1_train: 0.0572 loss_val: 35.4346 micro_F1_val: 0.0637\n",
      "Epoch: 0078 loss_train: 35.4804 micro_F1_train: 0.0537 loss_val: 35.3430 micro_F1_val: 0.0619\n",
      "Epoch: 0079 loss_train: 35.4015 micro_F1_train: 0.0514 loss_val: 35.2496 micro_F1_val: 0.0598\n",
      "Epoch: 0080 loss_train: 35.3082 micro_F1_train: 0.0542 loss_val: 35.1544 micro_F1_val: 0.0604\n",
      "Epoch: 0081 loss_train: 35.2216 micro_F1_train: 0.0522 loss_val: 35.0576 micro_F1_val: 0.0551\n",
      "Epoch: 0082 loss_train: 35.1236 micro_F1_train: 0.0537 loss_val: 34.9592 micro_F1_val: 0.0484\n",
      "Epoch: 0083 loss_train: 35.0297 micro_F1_train: 0.0502 loss_val: 34.8592 micro_F1_val: 0.0468\n",
      "Epoch: 0084 loss_train: 34.9383 micro_F1_train: 0.0498 loss_val: 34.7578 micro_F1_val: 0.0420\n",
      "Epoch: 0085 loss_train: 34.8382 micro_F1_train: 0.0458 loss_val: 34.6549 micro_F1_val: 0.0358\n",
      "Epoch: 0086 loss_train: 34.7412 micro_F1_train: 0.0457 loss_val: 34.5508 micro_F1_val: 0.0255\n",
      "Epoch: 0087 loss_train: 34.6398 micro_F1_train: 0.0487 loss_val: 34.4454 micro_F1_val: 0.0198\n",
      "Epoch: 0088 loss_train: 34.5421 micro_F1_train: 0.0455 loss_val: 34.3389 micro_F1_val: 0.0174\n",
      "Epoch: 0089 loss_train: 34.4386 micro_F1_train: 0.0436 loss_val: 34.2313 micro_F1_val: 0.0170\n",
      "Epoch: 0090 loss_train: 34.3285 micro_F1_train: 0.0403 loss_val: 34.1228 micro_F1_val: 0.0143\n",
      "Epoch: 0091 loss_train: 34.2330 micro_F1_train: 0.0396 loss_val: 34.0135 micro_F1_val: 0.0125\n",
      "Epoch: 0092 loss_train: 34.1276 micro_F1_train: 0.0408 loss_val: 33.9034 micro_F1_val: 0.0117\n",
      "Epoch: 0093 loss_train: 34.0224 micro_F1_train: 0.0392 loss_val: 33.7926 micro_F1_val: 0.0084\n",
      "Epoch: 0094 loss_train: 33.9093 micro_F1_train: 0.0355 loss_val: 33.6813 micro_F1_val: 0.0074\n",
      "Epoch: 0095 loss_train: 33.7971 micro_F1_train: 0.0325 loss_val: 33.5695 micro_F1_val: 0.0076\n",
      "Epoch: 0096 loss_train: 33.6931 micro_F1_train: 0.0347 loss_val: 33.4574 micro_F1_val: 0.0077\n",
      "Epoch: 0097 loss_train: 33.5868 micro_F1_train: 0.0327 loss_val: 33.3449 micro_F1_val: 0.0078\n",
      "Epoch: 0098 loss_train: 33.4729 micro_F1_train: 0.0323 loss_val: 33.2324 micro_F1_val: 0.0053\n",
      "Epoch: 0099 loss_train: 33.3723 micro_F1_train: 0.0304 loss_val: 33.1197 micro_F1_val: 0.0053\n",
      "Epoch: 0100 loss_train: 33.2514 micro_F1_train: 0.0321 loss_val: 33.0070 micro_F1_val: 0.0027\n",
      "Epoch: 0101 loss_train: 4.5315\n",
      "Epoch: 0102 loss_train: 4.0218\n",
      "Epoch: 0103 loss_train: 3.9023\n",
      "Epoch: 0104 loss_train: 3.9036\n",
      "Epoch: 0105 loss_train: 3.9710\n",
      "Epoch: 0106 loss_train: 3.9984\n",
      "Epoch: 0107 loss_train: 3.9480\n",
      "Epoch: 0108 loss_train: 3.8485\n",
      "Epoch: 0109 loss_train: 3.9098\n",
      "Epoch: 0110 loss_train: 3.7444\n",
      "Epoch: 0111 loss_train: 3.7431\n",
      "Epoch: 0112 loss_train: 3.7470\n",
      "Epoch: 0113 loss_train: 3.7233\n",
      "Epoch: 0114 loss_train: 3.7021\n",
      "Epoch: 0115 loss_train: 3.7180\n",
      "Epoch: 0116 loss_train: 3.7072\n",
      "Epoch: 0117 loss_train: 3.6905\n",
      "Epoch: 0118 loss_train: 3.6776\n",
      "Epoch: 0119 loss_train: 3.6687\n",
      "Epoch: 0120 loss_train: 3.6508\n",
      "Epoch: 0121 loss_train: 3.6522\n",
      "Epoch: 0122 loss_train: 3.6927\n",
      "Epoch: 0123 loss_train: 3.6390\n",
      "Epoch: 0124 loss_train: 3.6378\n",
      "Epoch: 0125 loss_train: 3.6532\n",
      "Epoch: 0126 loss_train: 3.6410\n",
      "Epoch: 0127 loss_train: 3.6122\n",
      "Epoch: 0128 loss_train: 3.5963\n",
      "Epoch: 0129 loss_train: 3.6504\n",
      "Epoch: 0130 loss_train: 3.6328\n",
      "Epoch: 0131 loss_train: 3.6035\n",
      "Epoch: 0132 loss_train: 3.6328\n",
      "Epoch: 0133 loss_train: 3.5893\n",
      "Epoch: 0134 loss_train: 3.5997\n",
      "Epoch: 0135 loss_train: 3.6105\n",
      "Epoch: 0136 loss_train: 3.6006\n",
      "Epoch: 0137 loss_train: 3.6313\n",
      "Epoch: 0138 loss_train: 3.5721\n",
      "Epoch: 0139 loss_train: 3.5906\n",
      "Epoch: 0140 loss_train: 3.5891\n",
      "Epoch: 0141 loss_train: 3.6099\n",
      "Epoch: 0142 loss_train: 3.5905\n",
      "Epoch: 0143 loss_train: 3.5878\n",
      "Epoch: 0144 loss_train: 3.5722\n",
      "Epoch: 0145 loss_train: 3.5711\n",
      "Epoch: 0146 loss_train: 3.5734\n",
      "Epoch: 0147 loss_train: 3.5853\n",
      "Epoch: 0148 loss_train: 3.5988\n",
      "Epoch: 0149 loss_train: 3.5651\n",
      "Epoch: 0150 loss_train: 3.5993\n",
      "Epoch: 0151 loss_train: 32.0517 micro_F1_train: 0.0191 loss_val: 31.7705 micro_F1_val: 0.0000\n",
      "Epoch: 0152 loss_train: 31.9088 micro_F1_train: 0.0226 loss_val: 31.6205 micro_F1_val: 0.0000\n",
      "Epoch: 0153 loss_train: 31.7660 micro_F1_train: 0.0169 loss_val: 31.4661 micro_F1_val: 0.0000\n",
      "Epoch: 0154 loss_train: 31.6056 micro_F1_train: 0.0190 loss_val: 31.3095 micro_F1_val: 0.0000\n",
      "Epoch: 0155 loss_train: 31.4515 micro_F1_train: 0.0135 loss_val: 31.1522 micro_F1_val: 0.0000\n",
      "Epoch: 0156 loss_train: 31.2938 micro_F1_train: 0.0160 loss_val: 30.9957 micro_F1_val: 0.0000\n",
      "Epoch: 0157 loss_train: 31.1201 micro_F1_train: 0.0151 loss_val: 30.8408 micro_F1_val: 0.0000\n",
      "Epoch: 0158 loss_train: 30.9669 micro_F1_train: 0.0141 loss_val: 30.6884 micro_F1_val: 0.0000\n",
      "Epoch: 0159 loss_train: 30.8195 micro_F1_train: 0.0138 loss_val: 30.5391 micro_F1_val: 0.0000\n",
      "Epoch: 0160 loss_train: 30.6643 micro_F1_train: 0.0124 loss_val: 30.3933 micro_F1_val: 0.0000\n",
      "Epoch: 0161 loss_train: 30.5112 micro_F1_train: 0.0107 loss_val: 30.2514 micro_F1_val: 0.0000\n",
      "Epoch: 0162 loss_train: 30.3596 micro_F1_train: 0.0104 loss_val: 30.1137 micro_F1_val: 0.0000\n",
      "Epoch: 0163 loss_train: 30.2196 micro_F1_train: 0.0092 loss_val: 29.9805 micro_F1_val: 0.0000\n",
      "Epoch: 0164 loss_train: 30.0865 micro_F1_train: 0.0081 loss_val: 29.8517 micro_F1_val: 0.0000\n",
      "Epoch: 0165 loss_train: 29.9468 micro_F1_train: 0.0093 loss_val: 29.7276 micro_F1_val: 0.0000\n",
      "Epoch: 0166 loss_train: 29.8245 micro_F1_train: 0.0075 loss_val: 29.6081 micro_F1_val: 0.0000\n",
      "Epoch: 0167 loss_train: 29.7078 micro_F1_train: 0.0085 loss_val: 29.4933 micro_F1_val: 0.0000\n",
      "Epoch: 0168 loss_train: 29.5762 micro_F1_train: 0.0080 loss_val: 29.3831 micro_F1_val: 0.0000\n",
      "Epoch: 0169 loss_train: 29.4675 micro_F1_train: 0.0064 loss_val: 29.2776 micro_F1_val: 0.0000\n",
      "Epoch: 0170 loss_train: 29.3511 micro_F1_train: 0.0040 loss_val: 29.1766 micro_F1_val: 0.0000\n",
      "Epoch: 0171 loss_train: 29.2582 micro_F1_train: 0.0061 loss_val: 29.0801 micro_F1_val: 0.0000\n",
      "Epoch: 0172 loss_train: 29.1603 micro_F1_train: 0.0047 loss_val: 28.9880 micro_F1_val: 0.0000\n",
      "Epoch: 0173 loss_train: 29.0631 micro_F1_train: 0.0046 loss_val: 28.9001 micro_F1_val: 0.0000\n",
      "Epoch: 0174 loss_train: 28.9678 micro_F1_train: 0.0029 loss_val: 28.8164 micro_F1_val: 0.0000\n",
      "Epoch: 0175 loss_train: 28.8849 micro_F1_train: 0.0058 loss_val: 28.7366 micro_F1_val: 0.0000\n",
      "Epoch: 0176 loss_train: 28.8039 micro_F1_train: 0.0037 loss_val: 28.6607 micro_F1_val: 0.0000\n",
      "Epoch: 0177 loss_train: 28.7187 micro_F1_train: 0.0037 loss_val: 28.5885 micro_F1_val: 0.0000\n",
      "Epoch: 0178 loss_train: 28.6388 micro_F1_train: 0.0036 loss_val: 28.5198 micro_F1_val: 0.0000\n",
      "Epoch: 0179 loss_train: 28.5807 micro_F1_train: 0.0029 loss_val: 28.4546 micro_F1_val: 0.0000\n",
      "Epoch: 0180 loss_train: 28.5106 micro_F1_train: 0.0031 loss_val: 28.3926 micro_F1_val: 0.0000\n",
      "Epoch: 0181 loss_train: 28.4483 micro_F1_train: 0.0026 loss_val: 28.3337 micro_F1_val: 0.0000\n",
      "Epoch: 0182 loss_train: 28.3829 micro_F1_train: 0.0024 loss_val: 28.2778 micro_F1_val: 0.0000\n",
      "Epoch: 0183 loss_train: 28.3315 micro_F1_train: 0.0032 loss_val: 28.2248 micro_F1_val: 0.0000\n",
      "Epoch: 0184 loss_train: 28.2733 micro_F1_train: 0.0024 loss_val: 28.1745 micro_F1_val: 0.0000\n",
      "Epoch: 0185 loss_train: 28.2267 micro_F1_train: 0.0014 loss_val: 28.1268 micro_F1_val: 0.0000\n",
      "Epoch: 0186 loss_train: 28.1726 micro_F1_train: 0.0019 loss_val: 28.0816 micro_F1_val: 0.0000\n",
      "Epoch: 0187 loss_train: 28.1259 micro_F1_train: 0.0010 loss_val: 28.0387 micro_F1_val: 0.0000\n",
      "Epoch: 0188 loss_train: 28.0822 micro_F1_train: 0.0021 loss_val: 27.9981 micro_F1_val: 0.0000\n",
      "Epoch: 0189 loss_train: 28.0373 micro_F1_train: 0.0005 loss_val: 27.9596 micro_F1_val: 0.0000\n",
      "Epoch: 0190 loss_train: 27.9951 micro_F1_train: 0.0005 loss_val: 27.9231 micro_F1_val: 0.0000\n",
      "Epoch: 0191 loss_train: 27.9594 micro_F1_train: 0.0010 loss_val: 27.8884 micro_F1_val: 0.0000\n",
      "Epoch: 0192 loss_train: 27.9285 micro_F1_train: 0.0017 loss_val: 27.8556 micro_F1_val: 0.0000\n",
      "Epoch: 0193 loss_train: 27.8920 micro_F1_train: 0.0015 loss_val: 27.8244 micro_F1_val: 0.0000\n",
      "Epoch: 0194 loss_train: 27.8595 micro_F1_train: 0.0010 loss_val: 27.7949 micro_F1_val: 0.0000\n",
      "Epoch: 0195 loss_train: 27.8238 micro_F1_train: 0.0010 loss_val: 27.7668 micro_F1_val: 0.0000\n",
      "Epoch: 0196 loss_train: 27.7925 micro_F1_train: 0.0012 loss_val: 27.7402 micro_F1_val: 0.0000\n",
      "Epoch: 0197 loss_train: 27.7732 micro_F1_train: 0.0007 loss_val: 27.7150 micro_F1_val: 0.0000\n",
      "Epoch: 0198 loss_train: 27.7459 micro_F1_train: 0.0003 loss_val: 27.6910 micro_F1_val: 0.0000\n",
      "Epoch: 0199 loss_train: 27.7218 micro_F1_train: 0.0003 loss_val: 27.6681 micro_F1_val: 0.0000\n",
      "Epoch: 0200 loss_train: 27.6949 micro_F1_train: 0.0009 loss_val: 27.6465 micro_F1_val: 0.0000\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(200):\n",
    "    train_sep(epoch, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'a.u.')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train_list,label='train')\n",
    "plt.plot(loss_val_list,label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('a.u.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'value')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABBS0lEQVR4nO3dd3hUZdr48e89k94I6SGhBEgg9CZSFcUGumJfsK7riq5lbbvvurpFt7z621197WKvKCI2VBQFaSq914QQWmgJgYQkpOf5/XEGCGESJiGTmUnuz3XNNZlznjNzHw1z5+lijEEppZSqy+bpAJRSSnknTRBKKaWc0gShlFLKKU0QSimlnNIEoZRSyilNEEoppZzSBKGUUsopTRBKuUBEOolIsYjYPR2LUi1FE4RSLjDG7DLGhBljqlvi80RkjIjUOJLSsceXjnN9RGS2iBwUEZ3pqtxGE4RSbiQifmdw+V5HUjr2+IXjeCUwHbjtzCNUqn6aIFSbJiI7ROQPIrJOREpE5A0RiReRb0SkSETmiEh7EekiIubYF76IRInIWyKyV0QOi8jnjuNjRCRHRP4oIvuBt0QkUESecZTd6/g5sKkxG2MyjDFvABub5T+CUvXQBKEUXA1cCKQBvwC+AR4BYrD+jfzOyTXvASFAbyAO+L9a5xKAKKAzMBl4FBgGDAD6A0OBPzf/bSjVvDRBKAXPG2MOGGP2AIuApcaY1caYcuAzYGDtwiKSCIwD7jTGHDbGVBpjFtQqUgP8zRhTbowpBW4A/m6MyTXG5AGPAze5EFcHESmo9biuGe5VKZedSfuoUq3FgVo/lzp5HVanfEfgkDHmcD3vl2eMKav1ugOws9brnY5jp7PXGJPsQjml3EJrEEo13m4gSkQi6zlfd2TRXqzmpmM6OY4p5dU0QSjVSMaYfVj9FC85OrD9ReScBi75EPiziMSKSAzwV+D9pn6+WIKAAMfroDPp9FaqPpoglGqam7CGm24BcoH7Gyj7T2AFsA5YD6xyHGuqzlhNX8dGMZUCGWfwfko5JbqjnFJKKWe0BqGUUsopTRBKeYiIPFJnKY1jj288HZtSoE1MSiml6tGq5kHExMSYLl26eDoMpZTyGStXrjxojIl1dq5VJYguXbqwYsUKT4ehlFI+Q0R21ndO+yCUUko5pQlCKaWUU5oglFJKOdWq+iCUUqqxKisrycnJoays7PSFfVhQUBDJycn4+/u7fI0mCKVUm5aTk0N4eDhdunRBRDwdjlsYY8jPzycnJ4eUlBSXr9MmJqVUm1ZWVkZ0dHSrTQ4AIkJ0dHSja0maIJRSbV5rTg7HNOUeNUGo+lUchSUvw64lUFNjHTMGcrfA0ldh+0KorvJsjEopt9E+CFW/7/8Cy1+3fg6Lhy6jYe9qOLTtRJngKOgxHs77E7TTzc+UaqyCggI++OAD7rrrrkZdN378eD744AMiIyPdExhag1D1yZpjJYezboer34BOwyF7PrTvApc+Db9bDde9B6kXwoZPYOa9no5YKZ9UUFDASy+9dMrx6urqBq+bNWuWW5MDaA2i9SvYZf3VH5MG0d3B7hjiVlkKYgM/JxuRlR6GL+6B2J5w0T/BPwj6XnNquaiu0OtySOgH3z0K236Abue7936UamUefvhhtm3bxoABA/D39ycsLIzExETWrFnDpk2buOKKK9i9ezdlZWXcd999TJ48GTixtFBxcTHjxo1j1KhR/PzzzyQlJfHFF18QHBx8xrFpgmjNdvwI066HskLrtc0fQmOgtACqSiEkGn79HcR0P/m6WX+AkjyY9KGVHE5n6O2w7BX4/q+QMgZsWjFVvunxLzeyae+RZn3PXh0i+Nsvetd7/sknn2TDhg2sWbOG+fPnc+mll7Jhw4bjw1HffPNNoqKiKC0t5ayzzuLqq68mOjr6pPfYunUrH374Ia+99hrXXXcdn3zyCTfeeOMZx64JorVa9zF8cZfVJPTL9+HIPsjdBCUHITgSgtvD4hdgxq/gtjknEsHSV2H9x3Deo9BhoGuf5RcI5/8VPv2NdW3/X7rpppRq/YYOHXrSXIXnnnuOzz77DIDdu3ezdevWUxJESkoKAwYMAGDw4MHs2LGjWWLRBOELqitPNA3VVrAbAkIhJOrEsZoaWPQUzPsndB4JE6daycCZ+N7w4USrM3r8f2DZa/DNH6DHpTDqwcbF2OdqWPw8/PBP6DXBtZqHUl6mob/0W0poaOjxn+fPn8+cOXNYvHgxISEhjBkzxulchsDAE03Fdrud0tLSZolF2wK8XcFueKonvHO51Z8A1lDT5a/D84PhuYGweqp1rOwITL/JSg59r4ObPqs/OQD0GAfD74Flr8Jnd8Ks31sjkq59G+yN/NvBZoML/w6Fu2DFG02+XaXamvDwcIqKipyeKywspH379oSEhLBlyxaWLFnSorFpDcKb1VTDp5OtDuU9K+GlEXDhY5C9ADbPtDqEK8uspqR1H8GRPXBoO1zyJJx9J7gyMWbs32Dnz7D2Q0gbB9e+A34BTYu36xhIOQd+ft4a/dTU91GqDYmOjmbkyJH06dOH4OBg4uPjj5+75JJLmDJlCv369aNHjx4MGzasRWNrVVuODhkyxLSqDYMW/sdqsrniZau56Iu7YccisPlZX+zD77HKrXwLvv+b1axz7dvQZVTjPufIXtj4GZz1G+ejmhojay68fxVMeBEGnnknmVLutnnzZtLT0z0dRotwdq8istIYM8RZea1BeKucFTDvCeh9FfSfZNUGbp5pdQLHpp3cgXzWbdD7SqtMQ01K9YnoAMPvbp64u50PCX3hp2eh//U6okkpH+bWf70icomIZIhIlog87OS8iMhzjvPrRGRQrXORIjJDRLaIyGYRGe7OWL1KcR58cpv1xX3Z/51oKrLZrBFCzkYXhUQ1LTk0NxEYeT8czITMb08cLyuEihKPhaWUajy3JQgRsQMvAuOAXsAkEelVp9g4INXxmAy8XOvcs8C3xpieQH9gs7ti9SolB+GdX0DRAWsGc3Bko99i+Y5DfLthf/PH5qpeV0BkJ/jpGasfZemr8HQva/KdUspnuLMGMRTIMsZkG2MqgGnAhDplJgDvGssSIFJEEkUkAjgHeAPAGFNhjClwY6zeoeSgNVrp8A64/iPodHaj38IYw4PT1/DbqSuZu/lA88foCrsfDL8Xdi+FKaOsobOmBnYt9kw8SqkmcWeCSAJ213qd4zjmSpmuQB7wloisFpHXRSQUJ0RksoisEJEVeXl5zRd9Sys7Au9OsBbCu34adD23SW+zfMdhdh8qJSzQj/unrSE7r7iZA3XRwBshNBaK9sOVr1oT74r2Wc1nSimf4M4E4WyMZd0hU/WV8QMGAS8bYwYCJcApfRgAxphXjTFDjDFDYmNjzyRezzHGGqGUuxkmfmANF22iT1flEBJg59PfjsDPLtzx3kqKyz2wJHdACNyxyFrUr/8vIbGfdXz/upaPRSnVJO5MEDlAx1qvk4G9LpbJAXKMMUsdx2dgJYzWacnL1ryGCx6D7mMbLLpq12F25R91eq6sspqv1+1jXJ9EUuPDefH6QWzLK+aPMzz0pRyReKIPJaGv9awJQqkzEhYW1mKf5c4EsRxIFZEUEQkAJgIz65SZCdzsGM00DCg0xuwzxuwHdotID0e5scAmN8bqObuXWUtd9LwMRjS8ZHZRWSWTXl3C+OcW8f2mU/sXvtt0gKLyKq4eZLXkjegew0MX9eDr9fuY46R8iwpuD+06wT5NEEr5CrclCGNMFXAPMBtrBNJ0Y8xGEblTRO50FJsFZANZwGtA7R0z7gWmisg6YADwv+6K1WPyMuDjX1kb7Ux48bQzn+duzqW8qoao0ABuf3cFz8zJpKbmRKvdp6ty6NAuiGFdTyzkNfmcrqTGhfHYlxsprWh4fXm3S+wH+9d7NgalvMwf//jHk/aDeOyxx3j88ccZO3YsgwYNom/fvnzxxRceic2tE+WMMbOwkkDtY1Nq/WwApzO0jDFrAKez+3xeTbW1kuoP/7IW25v0oUvDWb9at5fEdkHMvv8cHv18Pc/M2crS7EP8+bJ0YsMCWZiZx2/HdMNmO5Fo/O02/j6hD5NeW8JL87N46KIeDXyCmyX0hS1fQ3kxBLZcNVkpl33zcPP/EZPQF8Y9We/piRMncv/99x/fUW769Ol8++23PPDAA0RERHDw4EGGDRvG5Zdf3uJ7Z+tM6pZWkg/TJllDQHteZk2EC4s77WWFpZUszDzITcM7Exxg56lr+zOkcxT/mb2Fy57/kR7x4dQYuGrQqdt+Du8WzRUDOvDKgmyuGpRMSozTAWHul9APMNay4x2HeiYGpbzMwIEDyc3NZe/eveTl5dG+fXsSExN54IEHWLhwITabjT179nDgwAESEhJaNDZNEC1t9p9gzyq46jXoe61rC+oB3286QEV1DZf1SwRARLj+7E5c2i+Rl+Zn8dZPOxjSuT3dYp3/Zf7IpenM3ZzL32Zu5N1fN++X89rdBXSLCyMs8DS/Tsc6qvet1QShvFMDf+m70zXXXMOMGTPYv38/EydOZOrUqeTl5bFy5Ur8/f3p0qWL02W+3U0XymlJ2QusVVdH3Q/9rqs3OVRV1/Dawmy21ZrD8PW6vSRFBjOgY+RJZdsF+/Oncen89Mfzef2W+lvk4sKDuHdsdxZm5rFq1+FTzh84UsamvUdYkp3PupwCl2/phy0HmPDiT7w4L+v0hdslW53VOpJJqZNMnDiRadOmMWPGDK655hoKCwuJi4vD39+fefPmsXPnTo/EpQmipVSVw9cPWTu8jX6owaLPzNnKv2ZtZtKrS9iVf5SCoxUs2nqQy/ol1tsGGRseSGRIw8trX392ZyKC/HhtYfZJx6cu3cnZ/zuX8c8tYuKrS7j8hZ/4bHXOSWV2HCzhypd+YubaEyOV9xaU8uD0tQDM25Lb4GcDVkJM0I5qperq3bs3RUVFJCUlkZiYyA033MCKFSsYMmQIU6dOpWfPnh6JS5uYWspPz0H+VrjhE/CvfzPxeRm5vDAviwt7xbNixyGuf30J1w3pSFWN4bJ+Hc4ohLBAP24Y1pkpC7axM7+EztGh5BWV8+SsLQxNieLXI7sQEeTPU99n8tjMTYzsHkNceBAVVTXc++Fq1u8pZPWu1ew8WMId53bjng9WUVVtmDS0Ex8u28W+wlIS251mo/SEvtbOdfXtkqdUG7V+/Yk/nGJiYli82PnSNMXFLbc6gtYg3KmyFHYtgZ9fsPZ26H0lpF5Qb/E9BaU88NEa0hMjeH7SQN759VAKjlby9PeZdIoKoU9SxBmHdOuILvjbbLy+aDsA//52C2VV1TxxVV8u6ZPIiO4x/PuafpRVVvOXzzdgjOGp7zJYv6eQZycO4MqBSTz1fSaXPLOQVbsKePLqvtw6sgsA8zNcWEYjsT9Ul8PBrWd8L0op99IahDvU1MD8J+DHp6HGscxFbDpc/MQpRbceKGJbXjH7CsuYsTKHqmrDSzcMIsjfTr/kSN64ZQi3vLWMawcnN8sQt7iIIK4Y2IGPV+7mvJ6xfLwyhzvO7XpS53a32DAevDCNJ77ZwmMzN/LO4p1MGtqJCQOSuLx/B7pEh/J/czK5aVhnLuvXAWMMHdoFMT8jl0lDOzUcQO0Z1fF1F/dVSnkTTRDNrbwIPr0DMr6GPtdQkjaBDaRy2BbJRWEJJ1XZpq/Yzf/UWgYjNMDO078ccNIw1LO7RrP80QsIDWi+/1W3j+7K9BU53PHeSuLCA7n3/NRTytw2KoVZ6/fxzuKddI8L46+XWV/mIsJ9F6Ry1aAkkiKDjx8b0zOOL1bvoaKqhgC/Biqm0angF2TNqO4/sdnuSakzYYxp8TkGLa0pu4dqgmhOBbvhg+sweRnM7/p7/rFjFNkrjmJNFodJQzvyv1f2RUTYsKeQv3y+gRHdonlkfDodIoNpH+Lv9Jc0PKh52+pT48M5v2ccP2zJ5dFL050OT/Wz23jquv48NnMTf74sneAA+0nnO0aFnPR6TFosHyzdxYqdhxjRLab+D7f7QVwvHcmkvEZQUBD5+flER0e32iRhjCE/P5+goKBGXacJohmVzvw9toPbuaPqYeZv6s2o7iFcNSiZ/h0j+XlbPi/P30agn50HLkjjrqmraB8SwHOTBhITdob7QDfBny9NZ3jXaC7vX3/Hd/e4cN7/jWt7UozsHoO/XZifkddwggCI7QnZ8xoTrlJuk5ycTE5ODj69XYALgoKCSE4+dSJtQzRBNJMFi+Zzbva3vFBzNYkDxzFnVArd406064/qHkN5ZQ1v/rSd7zbuJ6+4nI/uGO6R5ADQNTaMrvVMqmuK0EA/hqZEMT8jl0fGn2YD+OhusPYDXXJDeQV/f39SUlI8HYZX0lFMZ6ikvIr/mbGWw9/9P0oJ4orJf+eJq/qelBzAaqf/y2XpXH92J/YWlvHo+HQGdfKCPaSb0Xk94sg8UMyegtJTzv3mneU85JgzQXR36/lQ9inllFLeQxNEE9XUGD5ZmcPYpxawbOUKLrcvIWD4bxqswokI/5zQhzkPnsstI7q0XLAtZEwPa8OmupPmduaXMGdzLp+symHDnkKrBgGQn0XmgSKWbT/U0qEqpVygTUzAGz9up2dCOGenROFnPzln5hWV8/nqPcxYmcOhoxWkxYeRGhfOql2HWZdTSL/kdrzRZSm2bf6n3c8BwGaTU2oXrUW32DC6xoYyY2UONw7rfPz4Z6v3IAJhAX48MyeT1ydZI6LK9mdy08wIyqtqWPnnC7HbWmcHoVK+qs0niKMVVTzzfSZF5VVEhwZwUe8EokMD2FNQSs7ho6zeVUBVjWFQp0j6JseyNbeY6St20y7Yn6ev688VXcH23Kcw+BYIb9mVFr2NiPCrEV346xcbWbXrMIM6tccYw2er9zAsJZqR3aP573eZrD2QSv+IJNatW8mBI9a8iDW7CxjcuXU1uSnl69p8gggJ8GPZoxcwPyOXWRv288WaPZRX1ZAQEURSZDC3jUrh2iHJdI8LP35NTY1BxPpC5PO7AAMj7/PcTXiRqwcl85/ZGbz10w4GdWrPql0F7Mw/yj3ndWdc30Te+HE7T3+fyVOByfgdyObXI1N4++ftLMjM0wShlJdp8wkCIDjAzri+iYzrm0hldQ02kQabO45vyLPuY1gz1Vp8L/I0M4jbiNBAPyae1ZE3f9rBI+N78umqHIL8bYzrm0hYoB93nNuNJ7/ZwtzAcMbZN/HI+J6s2X2YBRm5PHhhmqfDV0rVop3Udfjbba61hedvg6/uh47DYMwjbo/Ll9w8vAvGGN5YtJ2v1u3jol4Jxyfj3Ty8MzFhAWw3iUSYIvzKCxjTI451ewrJLy73cORKqdo0QTRFZRl8fIu1Guk1b1izg9VxHaNCuKhXAm/8tJ3C0kquHJR0/FxIgB9v/uosrr7wXOtAfhbnpsViDCzaetBDESulnNEE0RQ//MPa0+CKl61NcNQpbh3ZBWMgJiyQ0d1PnlndLzmS1PQB1ov8bfRNakd0aADzM1zYU0Ip1WL0T9/Gyt0CS16Gwb+CHuM8HY3XGpoSxSW9ExjUOfKUocMARHYGsUN+FjabcE5aLAsy86ipMSf6eJRSHuXWGoSIXCIiGSKSJSIPOzkvIvKc4/w6ERlU69wOEVkvImtEZIU743SZMfDtw9byEOf/1dPReDURYcpNg5l8TjfnBfwCoH1nyLe2Kh3TI5ZDJRWs31PYglEqpRritgQhInbgRWAc0AuYJCJ1NwAYB6Q6HpOBl+ucP88YM8AYU/9myy0p4xtrkbkxj0BotKej8X1R3eDQNgBGp8Yi4uKmQ0qpFuHOGsRQIMsYk22MqQCmARPqlJkAvGssS4BIEUl0Y0xNV1UOsx+BmB5w1m2ejqZ1iO5ujQYzhqjQAPolRzJz7R6W7zhETU3j165XSjUvd/ZBJAG7a73OAequHe2sTBKwDzDAdyJigFeMMa+6LdJD2VabuK3WngdH9sLy1+HwTmuDm+IDcHg73PiJ7qXcXKK7QeVRKNoHER349cgu/GHGOq6dspi48EBuHt6Ze5xsZqSUahnuTBDOehrr/lnYUJmRxpi9IhIHfC8iW4wxC0/5EJHJWM1TdOrUhMlq1VXw6hirw7TrGOh6LuxeDus+AlMN7btAVQVUlcJZv4Hu9e8prRqp1qJ9RHRgwoAkxqbH88OWXKYu2cl/v8vkqkHJdHDsXKeUalnuTBA5QMdar5OBva6WMcYce84Vkc+wmqxOSRCOmsWrAEOGDGl8u4SpgfFPwbYfYNtc2Pgp+AXDkFth+N1WglDucWzZ7/xtkHIOAGGBflzevwNdY0K57PkfWbb9EFcMTGrgTZRS7uLOBLEcSBWRFGAPMBG4vk6ZmcA9IjINq/mp0BizT0RCAZsxpsjx80XA390SpV8A9LvWehgDBzMhNBZCotzycaqWiGSwBx4fyVRbemIE4UF+LN2erwlCKQ9xWye1MaYKuAeYDWwGphtjNorInSJyp6PYLKwNm7OA14C7HMfjgR9FZC2wDPjaGPOtu2I9TgRie2hyaCk2m9XMlL/tlFN2mzC0SxRLs+vfK2Lt7gL6PTabrNwid0apVJvl1olyxphZWEmg9rEptX42wN1OrssG+rszNuUlYlJh52KoKIGA0JNODU2JYu6WXHKPlBEXcepm69NX7OZIWRUz1+zlwYt6tFTESrUZutSG8qyz74SSXFjw71NPdbXmmix1suNcVXUN327YD8DsjQfcG6NSbZQmCOVZnUfAgBtg8QuQu/mkU306RBAaYGfp9vxTLluSfYj8kgqGdY0i40AROw6WtFTESrUZmiCU5134dwgIg68fsgYKOPjZbQyupx/iq3V7CQ2w888rrB3pZm/c32LhKtVWaIJQnhcaAxc+Djt/grUfnnTq7JQotuYWn7RXRGV1Dd9u3M+FveLpHhdG7w4RmiCUcgNNEMo7DLwZkofC3H+cVIsY1tUaUbasVj/ET1kHKThayWX9OgBwce8EVu0qIPdIWcvGrFQrpwlCeQebDfr/Eor2QsGu44f7JkUS5G87qaP6q3X7CA/yY3Satc/Exb0TAPhuk3ZWK9WcNEEo79HBsdr73lXHDwX42RjcuT1LsvMpLq+irLKa2Rv3c1GvBAL9rLWz0uLD6BIdos1MSjUzTRDKe8T3AXsA7Fl10uHhXaPZsr+IPn+bTc+/fEtRWRWX9Tux6K+IcHHvBBZvy6ewtLKlo1aq1dId5ZT38AuAhL6nJIhbRnQhJiyQI2WVFJdXE+hnY3TqyduYXtIngVcWZvPpqhxuHZnSklEr1WppglDeJWkwrPkAaqqPL78eHuTPxKENr9Q7oGMkw7pG8eK8LH55VkdCAvRXW6kzpU1Myrt0GAQVxdaiiY0gIvzh4p4cLK7grZ92uCc2pdoYTRDKuyQNtp7rNDO5YnDn9lyQHscrC7ZReFT7IpQ6U5oglHeJ7g6BEbBnZZMuf+iiHhwpq+KVhaeuEKuUahxNEMq72GzQYcBJQ10bIz0xgsv7d+Ctn3aQW6QT55Q6E5oglPfpMAj2b4Cq8tOXdeL+C1Iprazm01V7mjkwpdoWTRDK+yQNhppKK0k0QddYa32muZt1ZrVSZ0IThPI+SafOqG6ssenxrNx5mMMlFc0UlFJtjyYI5X0ikiAsvskd1QAXpMdRY2BeRm4zBqZU26IJQnkfEasfoglDXY/p06EdceGBzN2sCUKpptIEobxTh4HWZLmKo0263GYTxqbHsSAzj4qqmmYOTqm2QROE8k6xPQAD+Vub/BZje8ZTXF510l4SSinXaYJQ3im2p/Wcl9HktxjZPYZAPxtzdDSTUk3i1gQhIpeISIaIZInIw07Oi4g85zi/TkQG1TlvF5HVIvKVO+NUXiiqK4j9jBJEcICdUd1jmLP5AKbWLnUAxhheX5SttQulGuC2BCEiduBFYBzQC5gkIr3qFBsHpDoek4GX65y/D9jsrhiVF/MLsJLEwaYnCLCGu+YcLiXzQPFJx5//IYt/fr2Z376/UofCKlUPd9YghgJZxphsY0wFMA2YUKfMBOBdY1kCRIpIIoCIJAOXAq+7MUblzWJ7QF7jVnWta2x6HH424b5pq8nYXwTA56v38PT3mZyTFkthaSX/mqV/gyjljDsTRBKwu9brHMcxV8s8A/wP0OAQFBGZLCIrRGRFXl7eGQWsvExMGhzaBtVNX5k1PiKIV28eTF5ROb944Uf+8dUm/mfGOs5OieK1mwcz+ZyuzFiZw49bDzZj4Eq1Du5MEOLkmHGljIhcBuQaY047U8oY86oxZogxZkhsbGxT4lTeKrYH1FTBoewzepvze8bz7f3nMLp7DG/8uJ3kqGBevWkIgX52fjc2lZSYUB75bD2lFdXNFLhSrYM7E0QO0LHW62Rgr4tlRgKXi8gOrKap80XkffeFqrxSTJr1fAYd1cfEhgfy+i1DeOOWIUy7fRjtQvwBCPK3879X9mXXoaO8ND/rjD9HqdbEnQliOZAqIikiEgBMBGbWKTMTuNkxmmkYUGiM2WeM+ZMxJtkY08Vx3Q/GmBvdGKvyRscSxBl2VB8jIoxNjycuIuik48O7RXNuWixfrdvXLJ+jVGvhtgRhjKkC7gFmY41Emm6M2Sgid4rInY5is4BsIAt4DbjLXfEoHxQYBu06nnFHtSvOTYtl+8ESdh9q2sxtpVojt+7sboyZhZUEah+bUutnA9x9mveYD8x3Q3jKF8SkNVsNoiGjU2MA+DHrIJOGdnL75ynlC3QmtfJux4a61rh3PaXucWHERwTqaCalatEEobxbTBpUlULh7tOXPQMiwujUWH7adpDqmrqD7ZRqmzRBKO8W28N6Puj+fojRqTEUHK1kw55Ct3+WUr5AE4Tybs2waJ+rRnY/0Q+hlNIEobxdSBSExLRIR3VMWCC9EiNYmKkz8pUCTRDKFzTDmkyuGp0Ww6pdhykpr2qRz1PKm2mCUN4vJg3ytoBxf+fx6O6xVFYblm7Pd/tnKeXtNEEo7xeTBmUFcNT9ezcM6dKeQD8bi3S4q1KaIJQPiEi0nov3u/2jgvztDOsazdzNuadsMqRUW3PaBCEi8SLyhoh843jdS0Ruc39oSjmEJVjPRe5PEADj+iSw69BRNu070iKfp5S3cqUG8TbWekodHK8zgfvdFI9SpwqLs56LW2Zv6Yt6J2C3CbPWn7x43zNzMrluymKdSKfaDFcSRIwxZjqOjXsci/Dpwvmq5YS3bA0iKjSAYV2jmLV+//FmpsKjlby6MJtlOw7xzQZd9VW1Da4kiBIRicax2c+xZbndGpVStQWEQkB4i9UgAMb1SWT7wRIyDljblH6wbBdHK6qJCw/k+blZ1GgtQrUBriSIB7H2begmIj8B7wL3ujUqpeoKj2+xGgTAxb0TsAnMWr+fiqoa3v55OyO7R/PopelkHCjiu00tF4tSnnLaBGGMWQWcC4wA7gB6G2PWuTswpU4SltCiNYjY8ECGpkQxa/0+vlq3lwNHyrl9dFcu69eBrjGhPDs3S0c5qVbPlVFMNwPXA4OBQcAkxzGlWk54fIsmCIDxfRPJyi3m399mkBYfxrlpsdhtwt3ndWfzviPM2ZzbovEo1dJcaWI6q9ZjNPAYcLkbY1LqVGEJUNSyCeLi3gmIwP4jZfxmVFdEBIAJAzrQOTqEJ77ZzM9ZB7UmoVotV5qY7q31uB0YCAS4PzSlagmLg8oSKC9qsY+MjwjirC5RxIYHMmFgh+PH/ew2Hru8N4dLKrj+9aVc/MxCPl+9x+l7aPJQvqwpM6mPAqnNHYhSDTo+1LVlaxHPThzA9DuGE+hnP+n4eT3iWPynsfz7mn7YRLj/ozWs3HnyUiALM/M4619zyD1S1pIhK9VsXOmD+FJEZjoeXwEZwBfuD02pWsLirecWWG6jtsR2waTEhDo9F+Rv57ohHfn0rhHEhAXw728zjtcYKqtrePzLjRwsrmBbXklLhqxUs/Fzocx/a/1cBew0xuS4KR6lnGvhyXKNERLgxz3ndeexLzfxY9ZBRqfG8uGyXccTw+GjFR6OUKmmcaUPYkGtx0+aHJRHHK9BeOfIoUlndyIpMpj/zM6gsLSS//s+k7T4MEAThPJd9SYIESkSkSNOHkUi4tIqZiJyiYhkiEiWiDzs5LyIyHOO8+tEZJDjeJCILBORtSKyUUQeb/otqlYhuD3YA1q8iclVgX527hubyrqcQm55cxkFpZU8eXU/AA6XaIJQvqneBGGMCTfGRDh5hBtjIk73xiJiB14ExgG9sOZP9KpTbBxWh3cqMBl42XG8HDjfGNMfGABc4ljiQ7VVIlYtooU7qRvjqkFJdI0JZc3uAq4amMygTu0JCbBz+Gilp0NTqklcHsUkInEi0unYw4VLhgJZxphsY0wFMA2YUKfMBOBdY1kCRIpIouN1saOMv+Oh4wXburB4r61BgDX89a+/6EWvxAh+f3EaAO1DArQGoXyWK6OYLheRrcB2YAGwA/jGhfdOAnbXep3jOOZSGRGxi8gaIBf43hiztJ74JovIChFZkZenm823auEtP1muscb0iGPWfaNJbBcMWCvDHtI+COWjXKlB/AMYBmQaY1KAscBPLlwnTo7VrQXUW8YYU22MGQAkA0NFpI+zDzHGvGqMGWKMGRIbG+tCWMpneXkNwpnIEH9tYlI+y5UEUWmMyQdsImIzxszD6hc4nRygY63XycDexpYxxhQA84FLXPhM1ZqFJ0DpYagq93QkLosK1SYm5btcSRAFIhIGLAKmisizWPMhTmc5kCoiKSISAEzEWja8tpnAzY7RTMOAQmPMPhGJFZFIABEJBi4Atrh2S6rV8vKhrs60DwnQYa7KZ7kyUW4hEAncB9wItAP+frqLjDFVInIP1nalduBNY8xGEbnTcX4KMAsYD2RhLeFxq+PyROAdx0goGzDdGPNVI+5LtUbHJssVH4DIjg2X9RLtQwIoKquisroGf3tTVrZRynNcSRCC9SV/CGsk0keOJqfTMsbMwkoCtY9NqfWzAe52ct06rEUBlTrh2N7UXjibuj5Rof6ANVkuLjzIw9Eo1TiuzKR+3BjTG+uLvAOwQETmuD0ypeoKO1aD8J0EERliLXxcoB3Vygc1ps6bC+wH8oE494SjVANCYwHx+qGutUWFWgnikHZUKx/kyjyI34rIfGAuEAPcbozp5+7AlDqF3c9KEj5Ug2jvqEHoSCbli1zpg+gM3G+MWePmWJQ6vXDvXm6jrvbH+yC0iUn5ntMmCGPMKYvsKeUxYQktvjf1mTheg9ChrsoH6bg75VvC4n0qQQT5260F+7SJSfkgTRDKt4THWxPlaqo9HYnL2ofoekzKN2mCUL4lLAFMNRx1aSqOV2gf6q81COWTNEEo33JsBvWh7Z6NoxGs5Ta0k1r5Hk0QyrfEpVvPuZs8G0cj6HpMyldpglC+pV0nCAjzqQQRFRqgE+WUT9IEoXyLzWbVIg74ToKovWCfUr5EE4TyPXHpVg3C+MYutMcmy+l6TMrXaIJQvieuN5Qe8pn5EO2PL9inzUzKt2iCUL4nvpf1fGCjZ+Nw0bEEof0QytdoglC+J86RIHI3ezYOF7WvtSeEUr5EE4TyPaExEBrnMyOZji35fWwuRHlVNU98s5mDxb6zt7ZqmzRBKN8U38tnm5jmZ+TxyoJsvlm/z5NhKXVamiCUb4rrDXlbfGJNpiB/O8H+9uOd1PMz8gDYsr/Ik2EpdVqaIJRvikuHqjI4vMPTkbjEmixXiTGGBRm5AGRoglBeThOE8k0+NpIpMsSfw0cryDxQzN7CMtoF+5NxoAjjI3M5VNukCUL5pth0QHyqo/rw0QrmO2oPNw7rRFFZFfsKyzwcmVL1c2uCEJFLRCRDRLJE5JSd6cTynOP8OhEZ5DjeUUTmichmEdkoIve5M07lgwJCICrFZ2oQ7UMCOFxSwfyMPHomhHNuWhwAGQe0mUl5L7clCBGxAy8C44BewCQR6VWn2Dgg1fGYDLzsOF4FPGSMSQeGAXc7uVa1dXG9fGcuRIg/B46Us3zHIcb0iKNHfDig/RDKu7mzBjEUyDLGZBtjKoBpwIQ6ZSYA7xrLEiBSRBKNMfuMMasAjDFFwGYgyY2xKl8U1wsObYPKUk9HclrtQwMoraymqsYwpkcs7UL8SYgIIlMThPJi7kwQScDuWq9zOPVL/rRlRKQLMBBY6uxDRGSyiKwQkRV5eXlnGrPyJfG9wNTAvnWejuS0jk2WCw/0Y3Dn9gCkJYTrUFfl1dyZIMTJsbpDNhosIyJhwCfA/caYI84+xBjzqjFmiDFmSGxsbJODVT6o63ngHwJr3vd0JKcV6ZgsNyo1Bn+79c+uZ0I4WXnFVOky4MpLuTNB5AAda71OBva6WkZE/LGSw1RjzKdujFP5quBI6HsNrPsYSgs8HU2Doh01iDE9TvwRkxYfTkVVDTvyjx4/9sOWA7oEh/Ia7kwQy4FUEUkRkQBgIjCzTpmZwM2O0UzDgEJjzD4REeANYLMx5mk3xqh83ZDboKoU1k7zdCQNOqtLFA+P68nl/U+0oPZMsDqqMx0jmdbnFPLrt1fw+iLf2W9btW5uSxDGmCrgHmA2VifzdGPMRhG5U0TudBSbBWQDWcBrwF2O4yOBm4DzRWSN4zHeXbEqH9ZhACQNgeWve/UGQgF+Nu48txvBAfbjx7rHhWGTE0tuvLwgC4BN+5y2pirV4vzc+ebGmFlYSaD2sSm1fjbA3U6u+xHn/RNKneqs38Dnd8L2hdD1XE9H47IgfztdokPJ2H+EbXnFfLNhP/52YYsmCOUldCa18n29r4Tg9rDiDU9H0mg9EsLJPFDMlPnbCPSz8ZvRXcktKidf+yGUF9AEoXyffxAMvBE2fwXbfvDqpqa60uLD2ZFfwmer9zDxrE6M7BYDwOZ9OvxVeZ4mCNU6DJ0MIdHw3pXw4tmw7DWfWAq8Z0L48Xx2+zldSU+0Oq43azOT8gKaIFTrENkJ7l8HV7wMAaEw6/dWkvByPRwjma4YmERSZDDRYYHEhQdqglBeQROEaj38g2HA9TB5HiQOgDVTPR3RaaXEhPLkVX15eFzP48fSEyPYrDOslRfQBKFap/4TYf86r1/MT0SYOLQTMWGBx4/1TAwnK7eIiqoTM6z3FpRSWFrpiRBVG6YJQrVOfa4BsXv9BDpneiVGUFlt2JZXDEB5VTWXv/AT459dxPaDJR6OTrUlmiBU6xQWC90vgHXTfaKzurb0xAjgREf17I3W8hv5JeVcO2Wx9k+oFqMJQrVe/X8JRXthxyJPR9IoXWNCCbDbjs+w/mDpTjpGBfPlPaPwswm/fGUxq3Yd9nCUqi3QBKFarx7jITAC1n7k6Ugaxc9uIzU+jM37jpCdV8yS7ENMPKsTqfHhfHzncNqHBnDzG8tYrUlCuZkmCNV6+QdDrwmw6Quo8K22+/TECDbvO8KHy3bhZxOuHZIMQMeoEKZNHkZUaAA3v7mM9TmFHo5UtWaaIFTr1n8iVJbAxs89HUmjpCdGcLC4gg+X7ebCXvHEhQcdP5fYLpgPJw+jXbA/N76xlBU7DmF8aPa48h2aIFTr1mkExPeF+U9CZZmno3HZsRnVxeVVTBra6ZTzSZHBfHj7MEID7FwzZTEjn/yBP85Yx/Idh1o6VNWKaYJQrZvNBhf/Ewp3wdKXPR2Ny9ITrJFMHaOCGdU9xmmZjlEhfPW70TxxVV/6d4xk1oZ93PDaUjbu1WYn1Tw0QajWr+sYSLsEFj0Nxb6xb3n70AAu65fIAxekYbPVv/J9VGgAk4Z24uUbB7PgD+fRPtSfez9YTUl5VQtGq1orTRCqbbjwH1ZH9fwnPB2Jy164fhBXDUp2uXxUaADPThzIjvwS/vL5BjdGptoKTRCqbYhNgyG/hpVve/3yG2diWNdofjc2lU9X72HGyhxPh6N8nCYI1XaM+RMERcA7v4CdP3s6Gre59/xUhnWN4m9fbCD3iO90zCvvowlCtR2h0XDrt9bkuXd+YS0H3gqHh9ptwhNX9aOiuob/zM7wdDjKh2mCUG1LXE+4/QfoNtbaM2Lxi56OyC1SYkK5dWQKM1bl6GQ61WSaIFTbExwJk6ZBp+Gw+j1PR+M295zfnaiQAB7/cqNOpFNNoglCtU02m7UMR94WyN/m6WjcIiLInz9c3IMVOw/z1bp9ng5H+SC3JggRuUREMkQkS0QednJeROQ5x/l1IjKo1rk3RSRXRHS8nnKPtEus58xvPRuHG107pCO9EiP4yxcb+OsXG/h+0wGKdY6EcpHbEoSI2IEXgXFAL2CSiPSqU2wckOp4TAZqT3V9G7jEXfEpRVQKxPWCjG88HYnb2G3CMxMHMKhTez5ekcPt765gxBNzydAtTZUL3FmDGApkGWOyjTEVwDRgQp0yE4B3jWUJECkiiQDGmIWALiyj3KvHOGvI69HW+6uWFh/Om786izV/u5D3bzubQH87k99bQeFR3cJUNcydCSIJ2F3rdY7jWGPLNEhEJovIChFZkZfnG8soKC/S41Iw1ZA1x9ORuF2gn51RqTFMuXEQewtK+d201VTXaOe1qp87E4SzBWTq/ja6UqZBxphXjTFDjDFDYmNjG3OpUtBhIITFQ8YsT0fSYgZ3juKxy3uzIDOPp78/dZ5EZXUNszfup7zKt7ZqVc3PnQkiB+hY63UysLcJZZRyH5vN6qzeOgeqKjwdTYu5fmgnJp7VkRfnbeOTOktyPP7lRu54byXPz83yUHTKW7gzQSwHUkUkRUQCgInAzDplZgI3O0YzDQMKjTE6Hk+1rB7joaLI5/auPhMiwuMTejOiWzR//GQdCzKt5tl3F+/g/SW7iAkL4PUfs9lfePJSHTqfom1xW4IwxlQB9wCzgc3AdGPMRhG5U0TudBSbBWQDWcBrwF3HrheRD4HFQA8RyRGR29wVq2rjup4LfsHWpLk29AUY6GfnlZsGkxofzm/fX8lrC7N5/MtNXJAezye/HUFNDTz13YkmqA17CjnrX3O56Y2l/LztoCaLNkBa0//kIUOGmBUrVng6DOWL5v4DFv0Xht8DF/0TpP49GFqb3CNlXPnSz+wpKKVnQjgzfjuCsEA//vX1Jl7/cTtf3zsaEZj02hIC/WxU18DB4nIGdIzkiav6kp4Y4elbUGdARFYaY4Y4PacJQimsmsOsP8Dy12DUgzD2r1BdYS0NfnArFOyAgl1wZC+UHLSGxUZ0gFtngc3u6ejPWHZeMS/My+LBC9NIbh8CQOHRSs75zzy6xISSc+go/nYbH90xjPiIIGaszOGZOZlEhQbw9e9G42/XRRl8lSYIpVxRUwNfP2DtGRHdHQ7vhJpacwVC46ykEBoLGGto7PUfQ9pFnorY7V5flM0/v95MXHggH90xnJSY0OPnvtu4n8nvreTR8encfk5XD0apzkRDCcKvpYNRymvZbHDp/0FwFOxbC+m/gIR+1mzryE4QEHKibFUFPJ0Oq95p1Qni5uFdOFpRzaX9Ek9KDgAX9orn/J5xPDMnk1/070BCuyAPRancRWsQSjXVd3+GJS/DA5sgPN7T0XjErvyjXPh/C7iwVzwvXD/o9Bcor9NQDUIbDpVqqoE3Q00VrP3A05F4TKfoEO4a052v1u3jx60HPR2OamaaIJRqqtg06DQCVr3bpobH1nXHuV1Jbh/Ms3MzTzm3cuchVuxovetctXaaIJQ6E4NuhkPZsONHT0fiMUH+dn41ogvLdxxmw54Tu9cVl1dx2zsruOH1pWzZf8Sl99p+sIQX52VRo2tEeQVNEEqdiV4TILCdNTz2wCbYtRT2rfN0VC3u2iEdCfa3887PO44fe2/xTgqOVhLoZ+OuqasocWEfihd+yOI/szOYs/mAG6NVrtIEodSZCAiBftfBpi/g5eHw5kXwymgrWbQh7YL9uWpQEl+s3cuhkgqOVlTx2qJszkmLZcqNg9l+sIS/fL6hwdnXZZXVfLdxPwDP/bBVZ2p7AU0QSp2p8x6BCS/BNW/BxA/A5g9rpno6qhZ3y4guVFTVMG35LqYu2cWhkgruG9udEd1j+N35qXy6eg/P/5BFxv4iKqpqTrl+fkYeReVVXN6/Axv2HGFeRq4H7kLVpvMglDpTIVEw8IYTr9MuhnUfwQWPgd3fY2G1tLT4cEZ0i+b9xTupqDaM7B7N4M5RAPxubCqrdh3m6e8zefr7TPxswgXp8bx4wyDsNmtZky/X7SU6NIB/X9OPVbsO8+zcLM7rEYe0oWVPvI3WIJRqbgNvhJI82PqdpyNpcbeM6MLewjIOFpdz39i048ftNuHtW4fyzX2jeXbiAK47qyPfbtzPR8ut/cJKyquYu/kA4/smEuRv5+7zurN2dwELdeisR2kNQqnm1v1Ca1mO1VOh56WejqZFXZAeT+foEJIigxmaEnXSObtNSE+MID0xgsv7d2BbbjH/nr2FcX0SWLg1j7LKGi4f0AGAqwcl8/zcrfx3dgZ5ReXUGINdhC4xoaTFhxEe1HZqZp6kCUKp5mb3g/6/tGZZF+dBWNvZ6dBuEz67ayQBfg03Thzbj+LS537kP99lcKCwjMR2QQzu1B6AAD8b95yfyiOfref3H6895fqOUcE8eGEaVwxI0iYoN9IEoZQ7DLgRfn7e6osYcY+no2lRUaEBLpXrmRDBzcM78/bPO7CL8OtRKdhsJ77sJw3tyDlpMRhjrb5eUVXDtrwStuYW8d3GAzzw0VpmbzjAv67sQ1RoAPuPlLEz/yh9k9oRGuj8q62wtJJHP1tPx6gQfn9Rj+P9H8o5XYtJKXd57XyoLIXf/tym9pdojCNllZz/3wUcLC7ny3tG0Te5nUvXVdcYXluUzdPfZRLkb8MARWXWPIuo0AB+e243bhzWmeCAE0ux7z50lFvfXk52XjE1Bs7vGcdzkwYSVk8yaSt0uW+lPGH5G/D1gzB5PnQY6OlovNa8Lbl8v/kA/7qiT6Obi7bsP8KL87YRGexPWnwYcRFBvL9kJ4u2HiQ2PJCxPePoHhdGVGgA//p6M1U1hik3DiYrr5jHZm4kNS6MN351FkmRwW66O++nCUIpTyg9DP/tAYN/BeP/7elo2pSl2flMWbCNtTmFHCqpAKBzdAhv/uosusWGAbAgM497pq4iItifj+4YdnyjpIbU1BiKyqpoF9J6Osk1QSjlKdNvgR2L4MEt4Oda27xqXvnF5ezILyEtPvyU0U8b9hQy6bUlRIUG8NHk4SS0C6K8qprpy3dTUlHN7aO7Hu+nqKiq4a6pK/l5Wz5f3TuKro5EczqFRysJDbTj56W77mmCUMpTMmfDB9dZM6zb2JBXX7F612FufH0p8e2CuGV4F15ZsI29hWUAXNQrnmcmDiDAbuPeD1fzzYb9BPnb6JccybTbh53UqV5XWWU1/+/bLbz10w4C7Da6xYXRMyGcawcnM6J7TEvd3mlpglDKU6orrZ3nOp4NE9ve8hu+Ytn2Q9zy5jJKK6sZ0DGShy5KIyu3mH98tYneHdrRKSqEr9fv4y+X9SIs0M4fP1nPv67sww1nd3b6fmt3F/Dg9DVsyyth0tBORAT7kbm/6HiT14hu0Tx0URp9kyIBqDGGnMNHydhfzLa8YtITI7ggvWVmkeuWo0p5it0f+l4Hy16Fo4esZTmU1xmaEsXHdw7nUEkFo1NjEBFGp8bSKSqEez9czfo9hfzxkp7cNioFYwxfrNnLE7O2cH7POBLbnejg3ltQyvM/ZDF9xW7iwgN5/7azGZV6orZQVlnNB0t38dL8LK5+eXGDMfVLbsdDF/XgHEc89VmSnc+GPYX8ZnTz7wvu1hqEiFwCPAvYgdeNMU/WOS+O8+OBo8CvjDGrXLnWGa1BKK+0fz1MGQXj/wtDb/d0NKqRMg8UkZVbzPi+iceP7co/ykXPLKB/ciTj+iQAsC2vhI+W78ZguH5oJx68qAftgp13Zh+tqOKz1XsoOFp5/FhCRBA9EsJJiQll1vp9PDt3KzmHS0lPjGDCgA5c1i/xpI70NbsL+O/sDH7MOkhSZDBzHzqXIH+7s49rkEeamETEDmQCFwI5wHJgkjFmU60y44F7sRLE2cCzxpizXbnWGU0Qymu9PBLsATB5nqcjUc3kvSU7+esXG45vJmi3CdcOTuae87u7NCLqdCqqapixMofpK3azZncBAHHhgYhYGxjmFpUTHRrAb8dYcz6akhzAc01MQ4EsY0y2I4hpwASg9pf8BOBdY2WpJSISKSKJQBcXrlXKd/SfBN89Ci8M1UlzrcRNwPVJBhwJQgRs+wWaaYvyAOB6x6OyQw1FZVVUVjuWSRfwj7IRGeKPbZ3A1ij49TfN88G1uDNBJAG7a73OwaolnK5MkovXAiAik4HJAJ06dTqziJVyl4E3wIGNUFni6UhUM2ra3+yN5w802HsV5NoM9MZyZ4Jw9mdS3fas+sq4cq110JhXgVfBamJqTIBKtZjg9nDly56OQqlGcWeCyAE61nqdDOx1sUyAC9cqpZRyI3dO7VsOpIpIiogEABOBmXXKzARuFsswoNAYs8/Fa5VSSrmR22oQxpgqEbkHmI3VVPemMWajiNzpOD8FmIU1gikLa5jrrQ1d665YlVJKnUpnUiulVBvW0DBX71w9SimllMdpglBKKeWUJgillFJOaYJQSinlVKvqpBaRPGBnEy+PAQ42Yzi+oC3eM7TN+26L9wxt874be8+djTGxzk60qgRxJkRkRX09+a1VW7xnaJv33RbvGdrmfTfnPWsTk1JKKac0QSillHJKE8QJr3o6AA9oi/cMbfO+2+I9Q9u872a7Z+2DUEop5ZTWIJRSSjmlCUIppZRTbT5BiMglIpIhIlki8rCn43EXEekoIvNEZLOIbBSR+xzHo0TkexHZ6nhu7+lYm5uI2EVktYh85XjdFu45UkRmiMgWx//z4a39vkXkAcfv9gYR+VBEglrjPYvImyKSKyIbah2r9z5F5E+O77cMEbm4MZ/VphOEiNiBF4FxQC9gkoj08mxUblMFPGSMSQeGAXc77vVhYK4xJhWY63jd2twHbK71ui3c87PAt8aYnkB/rPtvtfctIknA74Ahxpg+WNsETKR13vPbwCV1jjm9T8e/8YlAb8c1Lzm+91zSphMEMBTIMsZkG2MqgGnABA/H5BbGmH3GmFWOn4uwvjCSsO73HUexd4ArPBKgm4hIMnAp8Hqtw639niOAc4A3AIwxFcaYAlr5fWPtbxMsIn5ACNYulK3uno0xC4FDdQ7Xd58TgGnGmHJjzHasvXeGuvpZbT1BJAG7a73OcRxr1USkCzAQWArEO3bxw/Ec58HQ3OEZ4H+AmlrHWvs9dwXygLccTWuvi0gorfi+jTF7gP8Cu4B9WLtTfkcrvuc66rvPM/qOa+sJQpwca9XjfkUkDPgEuN8Yc8TT8biTiFwG5BpjVno6lhbmBwwCXjbGDARKaB1NK/VytLlPAFKADkCoiNzo2ai8whl9x7X1BJEDdKz1OhmrWtoqiYg/VnKYaoz51HH4gIgkOs4nArmeis8NRgKXi8gOrObD80XkfVr3PYP1e51jjFnqeD0DK2G05vu+ANhujMkzxlQCnwIjaN33XFt993lG33FtPUEsB1JFJEVEArA6c2Z6OCa3EBHBapPebIx5utapmcAtjp9vAb5o6djcxRjzJ2NMsjGmC9b/2x+MMTfSiu8ZwBizH9gtIj0ch8YCm2jd970LGCYiIY7f9bFY/Wyt+Z5rq+8+ZwITRSRQRFKAVGCZy+9qjGnTD2A8kAlsAx71dDxuvM9RWFXLdcAax2M8EI016mGr4znK07G66f7HAF85fm719wwMAFY4/n9/DrRv7fcNPA5sATYA7wGBrfGegQ+x+lkqsWoItzV0n8Cjju+3DGBcYz5Ll9pQSinlVFtvYlJKKVUPTRBKKaWc0gShlFLKKU0QSimlnNIEoZRSyilNEEp5AREZc2y1WaW8hSYIpZRSTmmCUKoRRORGEVkmImtE5BXHXhPFIvKUiKwSkbkiEusoO0BElojIOhH57Nga/SLSXUTmiMhaxzXdHG8fVmsPh6mOGcFKeYwmCKVcJCLpwC+BkcaYAUA1cAMQCqwyxgwCFgB/c1zyLvBHY0w/YH2t41OBF40x/bHWC9rnOD4QuB9rb5KuWGtJKeUxfp4OQCkfMhYYDCx3/HEfjLUoWg3wkaPM+8CnItIOiDTGLHAcfwf4WETCgSRjzGcAxpgyAMf7LTPG5DherwG6AD+6/a6UqocmCKVcJ8A7xpg/nXRQ5C91yjW0fk1DzUbltX6uRv99Kg/TJialXDcXuEZE4uD4PsCdsf4dXeMocz3wozGmEDgsIqMdx28CFhhrD44cEbnC8R6BIhLSkjehlKv0LxSlXGSM2SQifwa+ExEb1mqad2NtyNNbRFYChVj9FGAtuzzFkQCygVsdx28CXhGRvzve49oWvA2lXKaruSp1hkSk2BgT5uk4lGpu2sSklFLKKa1BKKWUckprEEoppZzSBKGUUsopTRBKKaWc0gShlFLKKU0QSimlnPr/Ix8S0/YwNLkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(micro_F1_train_list,label='train')\n",
    "plt.plot(micro_F1_val_list,label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('micro_F1')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    low_layer.eval()\n",
    "    output_ll, X_embedding_ = low_layer(Y_, X, E_tilde, A_tilde)\n",
    "    loss_test = np.sum([F.binary_cross_entropy_with_logits(output_ll[idx_test][:,i], labels[idx_test][:,i]) for i in range(39)])\n",
    "    micro_F1_train = micro_F1(threshold(output_ll.detach().numpy()[idx_test]), labels.detach().numpy()[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"micro_F1= {:.4f}\".format(micro_F1_train.item()))\n",
    "    return threshold(output_ll.detach().numpy()[idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 27.6970 micro_F1= 0.0000\n"
     ]
    }
   ],
   "source": [
    "y_predict = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = y_predict.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.00      0.00      0.00        25\n",
      "     class 2       0.00      0.00      0.00        76\n",
      "     class 3       0.00      0.00      0.00        59\n",
      "     class 4       0.00      0.00      0.00        28\n",
      "     class 5       0.00      0.00      0.00        85\n",
      "     class 6       0.00      0.00      0.00        80\n",
      "     class 7       0.00      0.00      0.00        52\n",
      "     class 8       0.00      0.00      0.00       169\n",
      "     class 9       0.00      0.00      0.00        28\n",
      "    class 10       0.00      0.00      0.00        46\n",
      "    class 11       0.00      0.00      0.00        52\n",
      "    class 12       0.00      0.00      0.00         4\n",
      "    class 13       0.00      0.00      0.00        50\n",
      "    class 14       0.00      0.00      0.00        34\n",
      "    class 15       0.00      0.00      0.00        12\n",
      "    class 16       0.00      0.00      0.00        29\n",
      "    class 17       0.00      0.00      0.00        48\n",
      "    class 18       0.00      0.00      0.00        43\n",
      "    class 19       0.00      0.00      0.00        97\n",
      "    class 20       0.00      0.00      0.00        23\n",
      "    class 21       0.00      0.00      0.00        30\n",
      "    class 22       0.00      0.00      0.00        27\n",
      "    class 23       0.00      0.00      0.00        34\n",
      "    class 24       0.00      0.00      0.00        98\n",
      "    class 25       0.00      0.00      0.00        30\n",
      "    class 26       0.00      0.00      0.00        32\n",
      "    class 27       0.00      0.00      0.00         9\n",
      "    class 28       0.00      0.00      0.00        16\n",
      "    class 29       0.00      0.00      0.00        13\n",
      "    class 30       0.00      0.00      0.00        37\n",
      "    class 31       0.00      0.00      0.00        11\n",
      "    class 32       0.00      0.00      0.00        33\n",
      "    class 33       0.00      0.00      0.00        12\n",
      "    class 34       0.00      0.00      0.00         6\n",
      "    class 35       0.00      0.00      0.00         7\n",
      "    class 36       0.00      0.00      0.00        16\n",
      "    class 37       0.00      0.00      0.00         6\n",
      "    class 38       0.00      0.00      0.00         5\n",
      "    class 39       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      1463\n",
      "   macro avg       0.00      0.00      0.00      1463\n",
      "weighted avg       0.00      0.00      0.00      1463\n",
      " samples avg       0.00      0.00      0.00      1463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/linheshan/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_true = labels.numpy()[idx_test]\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9',\n",
    "                'class 10', 'class 11', 'class 12', 'class 13', 'class 14', 'class 15', 'class 16', 'class 17', 'class 18', 'class 19',\n",
    "                'class 20', 'class 21', 'class 22', 'class 23', 'class 24', 'class 25', 'class 26', 'class 27', 'class 28', 'class 29',\n",
    "                'class 30', 'class 31', 'class 32', 'class 33', 'class 34', 'class 35', 'class 36', 'class 37', 'class 38', 'class 39']\n",
    "print(classification_report(y_true, y_predict, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
